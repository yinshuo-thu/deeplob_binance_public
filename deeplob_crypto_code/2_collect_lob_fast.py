#!/usr/bin/env python3
"""
DeepLOB æ•°æ®é‡‡é›†ç³»ç»Ÿ - æé€Ÿç‰ˆ
ä¼˜åŒ–ç­–ç•¥ï¼š
1. åˆ†ç¦»ä¸‹è½½å’Œå¤„ç†é˜¶æ®µ
2. ç›´æ¥ä¿å­˜ä¸ºnpyæ ¼å¼ï¼ˆè·³è¿‡Parquetï¼‰
3. æœ€å¤§åŒ–å¹¶è¡Œå¤„ç†ï¼ˆä½¿ç”¨å…¨éƒ¨CPUæ ¸å¿ƒï¼‰
4. æ‰¹é‡å¤„ç†å‡å°‘I/O
5. å†…å­˜ä¼˜åŒ–çš„å‘é‡åŒ–å¤„ç†
"""
import pandas as pd
import numpy as np
import gzip
import json
import os
from pathlib import Path
from datetime import datetime
from tardis_dev import datasets
import time
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import multiprocessing as mp
import warnings
warnings.filterwarnings('ignore')

# æ¸…é™¤ä»£ç†
for proxy_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
    if proxy_var in os.environ:
        del os.environ[proxy_var]

print(f"ğŸŒ ç›´è¿æ¨¡å¼: ä¸ä½¿ç”¨ä»£ç†")

# ================= é…ç½®åŠ è½½ =================

def load_config(config_path='2_config.json'):
    """åŠ è½½JSONé…ç½®æ–‡ä»¶"""
    # å¦‚æœä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºè„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = Path(__file__).parent
    config_file = script_dir / config_path
    with open(config_file, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config

CONFIG = load_config()

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„
SCRIPT_DIR = Path(__file__).parent

API_KEY = CONFIG['api']['key']
EXCHANGE = CONFIG['api']['exchange']
DATA_TYPE = CONFIG['api']['data_type']
FROM_DATE = CONFIG['data']['from_date']
TO_DATE = CONFIG['data']['to_date']
NUM_SYMBOLS = CONFIG['data']['num_symbols']
# å¤„ç†ç›¸å¯¹è·¯å¾„ï¼šå¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºè„šæœ¬ç›®å½•
candidates_path = CONFIG['data']['candidates_file']
CANDIDATES_FILE = SCRIPT_DIR / candidates_path if not Path(candidates_path).is_absolute() else Path(candidates_path)
LOB_LEVELS = CONFIG['processing']['lob_levels']

# æœ€å¤§åŒ–CPUåˆ©ç”¨
CPU_COUNT = mp.cpu_count()
DOWNLOAD_WORKERS = 1  # Tardis APIé™åˆ¶
PROCESS_WORKERS = min(CPU_COUNT - 4, 96)  # ä¿ç•™4æ ¸ç»™ç³»ç»Ÿ

OUTPUT_DIR = Path(CONFIG['paths']['output_dir'])
DOWNLOAD_DIR = Path(CONFIG['paths']['download_dir'])
OUTPUT_DIR.mkdir(exist_ok=True)
DOWNLOAD_DIR.mkdir(exist_ok=True)

# ================= æ ¸å¿ƒå¤„ç†å‡½æ•° =================

def process_csv_ultra_fast(csv_path, lob_levels=10):
    """
    è¶…å¿«é€Ÿå¤„ç†å•ä¸ªCSVæ–‡ä»¶ - ç›´æ¥è¾“å‡ºnpyæ ¼å¼
    è¿”å›: (N, 43)æ•°ç»„ï¼ŒåŒ…å«40ä¸ªç‰¹å¾ + timestamp + return_10s + return_60s
    """
    try:
        # è¯»å–CSV
        with gzip.open(csv_path, 'rt') as f:
            df = pd.read_csv(f)
        
        if len(df) == 0:
            return None
        
        # ä¸‹é‡‡æ ·åˆ°1ç§’
        df['timestamp_sec'] = pd.to_datetime(df['local_timestamp'], unit='us').dt.floor('1s')
        df_1s = df.groupby('timestamp_sec', as_index=False).last()
        
        n = len(df_1s)
        if n < 100:  # éœ€è¦è‡³å°‘100ä¸ªæ ·æœ¬
            return None
        
        # é¢„åˆ†é…æ•°ç»„ (timestamp + 40 features + 2 returns)
        data = np.zeros((n, 43), dtype=np.float32)
        
        # æ—¶é—´æˆ³ï¼ˆè½¬ä¸ºUnixæ—¶é—´æˆ³ï¼‰
        data[:, 0] = df_1s['timestamp_sec'].astype(np.int64) // 10**9
        
        # æå–ç‰¹å¾ - å‘é‡åŒ–æ“ä½œ
        col_idx = 1
        for i in range(lob_levels):
            data[:, col_idx] = df_1s[f'asks[{i}].price'].values
            col_idx += 1
        for i in range(lob_levels):
            data[:, col_idx] = df_1s[f'asks[{i}].amount'].values
            col_idx += 1
        for i in range(lob_levels):
            data[:, col_idx] = df_1s[f'bids[{i}].price'].values
            col_idx += 1
        for i in range(lob_levels):
            data[:, col_idx] = df_1s[f'bids[{i}].amount'].values
            col_idx += 1
        
        # è®¡ç®—ä¸­é—´ä»·
        mid_price = (df_1s['asks[0].price'].values + df_1s['bids[0].price'].values) / 2.0
        
        # è®¡ç®—return - å‘é‡åŒ–
        # return_10s
        k_10s = 10
        returns_10s = np.zeros(n, dtype=np.float32)
        valid_10s = np.arange(n - k_10s)
        returns_10s[valid_10s] = (mid_price[valid_10s + k_10s] - mid_price[valid_10s]) / mid_price[valid_10s]
        data[:, 41] = returns_10s
        
        # return_60s
        k_60s = 60
        returns_60s = np.zeros(n, dtype=np.float32)
        valid_60s = np.arange(n - k_60s)
        returns_60s[valid_60s] = (mid_price[valid_60s + k_60s] - mid_price[valid_60s]) / mid_price[valid_60s]
        data[:, 42] = returns_60s
        
        # ç§»é™¤æœ€å60ä¸ªæ ·æœ¬
        data = data[:-k_60s]
        
        return data
        
    except Exception as e:
        return None


def process_symbol_batch(args):
    """æ‰¹é‡å¤„ç†å•ä¸ªæ ‡çš„çš„æ‰€æœ‰æ–‡ä»¶"""
    symbol, download_dir, output_dir, lob_levels = args
    start_time = time.time()
    
    try:
        # æŸ¥æ‰¾è¯¥æ ‡çš„çš„æ‰€æœ‰CSVæ–‡ä»¶
        pattern = f"*_{symbol}.csv.gz"
        csv_files = sorted(download_dir.glob(pattern))
        
        if not csv_files:
            return {'symbol': symbol, 'status': 'no_files', 'time': 0}
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶
        all_data = []
        for csv_file in csv_files:
            data = process_csv_ultra_fast(csv_file, lob_levels)
            if data is not None:
                all_data.append(data)
        
        if not all_data:
            return {'symbol': symbol, 'status': 'failed', 'time': time.time() - start_time}
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        final_data = np.vstack(all_data)
        
        # ä¿å­˜ä¸ºnpy
        date_range = f"{FROM_DATE.replace('-', '')}_{TO_DATE.replace('-', '')}"
        output_file = output_dir / f"{symbol}_{date_range}.npy"
        np.save(str(output_file), final_data)
        
        # æ¸…ç†åŸå§‹æ–‡ä»¶
        for csv_file in csv_files:
            csv_file.unlink()
        
        return {
            'symbol': symbol,
            'status': 'success',
            'samples': len(final_data),
            'files': len(csv_files),
            'size_mb': output_file.stat().st_size / (1024**2),
            'time': time.time() - start_time
        }
        
    except Exception as e:
        return {'symbol': symbol, 'status': 'error', 'error': str(e), 'time': time.time() - start_time}


# ================= ä¸»ç¨‹åº =================

def load_symbols():
    """è¯»å–æ ‡çš„åˆ—è¡¨"""
    symbols = []
    try:
        with open(CANDIDATES_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    symbols.append(line)
                    if len(symbols) >= NUM_SYMBOLS:
                        break
        return symbols
    except Exception as e:
        print(f"âš ï¸  è¯»å–æ ‡çš„æ–‡ä»¶å¤±è´¥: {e}")
        return []


def main():
    """ä¸»ç¨‹åº"""
    print("=" * 70)
    print("DeepLOB æ•°æ®é‡‡é›†ç³»ç»Ÿ - æé€Ÿç‰ˆ")
    print("=" * 70)
    print(f"\nâ° ç¨‹åºå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ–¥ï¸  CPUæ ¸å¿ƒæ•°: {CPU_COUNT}")
    print(f"âš™ï¸  ä¸‹è½½å¹¶å‘: {DOWNLOAD_WORKERS}")
    print(f"âš™ï¸  å¤„ç†å¹¶å‘: {PROCESS_WORKERS}")
    print(f"ğŸ“… æ•°æ®èŒƒå›´: {FROM_DATE} è‡³ {TO_DATE}")
    
    # è¯»å–æ ‡çš„
    symbols = load_symbols()
    if not symbols:
        print("\nâŒ é”™è¯¯: æœªæ‰¾åˆ°å¯ç”¨çš„æ ‡çš„")
        return
    
    print(f"\nğŸ“Š å¤„ç†æ ‡çš„: {len(symbols)} ä¸ª")
    for i, symbol in enumerate(symbols[:10], 1):
        print(f"    {i:2d}. {symbol}")
    if len(symbols) > 10:
        print(f"    ... å…± {len(symbols)} ä¸ª")
    
    # ================= é˜¶æ®µ1: æé€Ÿä¸‹è½½ =================
    
    print(f"\n{'=' * 70}")
    print("[é˜¶æ®µ 1/2] æé€Ÿä¸‹è½½ï¼ˆå•çº¿ç¨‹ï¼ŒTardis APIé™åˆ¶ï¼‰")
    print("-" * 70)
    
    download_start = time.time()
    print(f"â° ä¸‹è½½å¼€å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“¥ æ­£åœ¨ä» Tardis.dev ä¸‹è½½...")
    
    try:
        datasets.download(
            exchange=EXCHANGE,
            data_types=[DATA_TYPE],
            from_date=FROM_DATE,
            to_date=TO_DATE,
            symbols=symbols,
            api_key=API_KEY,
            download_dir=str(DOWNLOAD_DIR)
        )
    except Exception as e:
        print(f"   âš ï¸  éƒ¨åˆ†æ•°æ®ä¸‹è½½å¤±è´¥: {e}")
        print(f"   â„¹ï¸  ç»§ç»­å¤„ç†å·²ä¸‹è½½çš„æ•°æ®...")
    
    download_time = time.time() - download_start
    all_files = list(DOWNLOAD_DIR.glob("*.csv.gz"))
    total_size = sum(f.stat().st_size for f in all_files) / (1024**2) if all_files else 0
    
    print(f"\nâœ… ä¸‹è½½å®Œæˆ")
    print(f"â° ä¸‹è½½ç»“æŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â±ï¸  ä¸‹è½½ç”¨æ—¶: {download_time/60:.1f} åˆ†é’Ÿ")
    print(f"ğŸ“ æ–‡ä»¶æ•°: {len(all_files):,}")
    print(f"ğŸ’¾ æ€»å¤§å°: {total_size:.2f} MB ({total_size/1024:.2f} GB)")
    
    if len(all_files) == 0:
        print("\nâŒ é”™è¯¯: æ²¡æœ‰ä¸‹è½½åˆ°ä»»ä½•æ–‡ä»¶")
        return
    
    # ================= é˜¶æ®µ2: è¶…é«˜é€Ÿå¹¶è¡Œå¤„ç† =================
    
    print(f"\n{'=' * 70}")
    print(f"[é˜¶æ®µ 2/2] è¶…é«˜é€Ÿå¹¶è¡Œå¤„ç† ({PROCESS_WORKERS} è¿›ç¨‹)")
    print("-" * 70)
    
    processing_start = time.time()
    print(f"â° å¤„ç†å¼€å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ’¾ è¾“å‡ºæ ¼å¼: NPY (NumPyåŸç”Ÿæ ¼å¼)")
    print()
    
    process_args = [
        (symbol, DOWNLOAD_DIR, OUTPUT_DIR, LOB_LEVELS)
        for symbol in symbols
    ]
    
    results = []
    completed = 0
    
    with ProcessPoolExecutor(max_workers=PROCESS_WORKERS) as executor:
        futures = {executor.submit(process_symbol_batch, args): args[0] for args in process_args}
        
        for future in as_completed(futures):
            symbol = futures[future]
            try:
                result = future.result()
                results.append(result)
                completed += 1
                
                if result['status'] == 'success':
                    print(f"  âœ“ [{completed:2d}/{len(symbols)}] {symbol:15s} | "
                          f"{result['samples']:8,} æ ·æœ¬ | "
                          f"{result['size_mb']:7.2f} MB | "
                          f"{result['time']:5.1f}s")
                elif result['status'] == 'no_files':
                    print(f"  âŠ˜ [{completed:2d}/{len(symbols)}] {symbol:15s} | è·³è¿‡ï¼ˆæ— æ•°æ®ï¼‰")
                else:
                    print(f"  âŠ˜ [{completed:2d}/{len(symbols)}] {symbol:15s} | è·³è¿‡ï¼ˆå¤±è´¥ï¼‰")
                        
            except Exception as e:
                print(f"  âœ— [{completed:2d}/{len(symbols)}] {symbol:15s} | å¼‚å¸¸: {e}")
                completed += 1
    
    processing_time = time.time() - processing_start
    total_time = time.time() - download_start + download_time
    
    print(f"\nâ° å¤„ç†ç»“æŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â±ï¸  å¤„ç†ç”¨æ—¶: {processing_time/60:.1f} åˆ†é’Ÿ")
    
    # ================= ç»Ÿè®¡æŠ¥å‘Š =================
    
    successful = [r for r in results if r['status'] == 'success']
    
    print(f"\n{'=' * 70}")
    print("å¤„ç†å®Œæˆ - ç»Ÿè®¡æŠ¥å‘Š")
    print("=" * 70)
    
    print(f"\nğŸ“Š æ€»ä½“æƒ…å†µ:")
    print(f"   å¤„ç†æ ‡çš„æ•°: {len(results)}")
    print(f"   æˆåŠŸ: {len(successful)}")
    print(f"   è·³è¿‡: {len(results) - len(successful)}")
    
    if successful:
        total_samples = sum(r['samples'] for r in successful)
        total_size_mb = sum(r['size_mb'] for r in successful)
        avg_time = np.mean([r['time'] for r in successful])
        
        print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {total_samples:,}")
        print(f"   æ€»æ–‡ä»¶å¤§å°: {total_size_mb:.2f} MB ({total_size_mb/1024:.2f} GB)")
        print(f"   å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f} ç§’/æ ‡çš„")
        
        print(f"\nâ±ï¸  æ—¶é—´ç»Ÿè®¡:")
        print(f"   ä¸‹è½½æ—¶é—´: {download_time/60:.1f} åˆ†é’Ÿ")
        print(f"   å¤„ç†æ—¶é—´: {processing_time/60:.1f} åˆ†é’Ÿ")
        print(f"   æ€»ç”¨æ—¶: {total_time/60:.1f} åˆ†é’Ÿ ({total_time/3600:.2f} å°æ—¶)")
        
        print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
        throughput = total_samples / processing_time
        print(f"   ååé‡: {throughput:,.0f} æ ·æœ¬/ç§’")
        print(f"   å¹¶è¡Œæ•ˆç‡: {len(successful) / processing_time:.2f} æ ‡çš„/ç§’")
        print(f"   CPUåˆ©ç”¨ç‡: {PROCESS_WORKERS}/{CPU_COUNT} ({PROCESS_WORKERS/CPU_COUNT*100:.1f}%)")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'date_range': f"{FROM_DATE.replace('-', '')}_{TO_DATE.replace('-', '')}",
            'total_symbols': len(successful),
            'total_samples': int(total_samples),
            'feature_dim': 40,
            'lob_levels': LOB_LEVELS,
            'target_col': 'return_10s',
            'additional_targets': ['return_60s'],
            'symbols': [r['symbol'] for r in successful],
            'download_date': datetime.now().isoformat(),
            'processing_time_minutes': round(total_time / 60, 2),
            'format': 'npy',
            'data_shape': '(N, 43) - [timestamp, 40 features, return_10s, return_60s]'
        }
        
        metadata_file = OUTPUT_DIR / 'metadata.json'
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"\nğŸ“„ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")
    
    # æ¸…ç†å‰©ä½™åŸå§‹æ–‡ä»¶
    remaining = list(DOWNLOAD_DIR.glob("*.csv.gz"))
    if remaining:
        print(f"\nğŸ—‘ï¸  æ¸…ç†å‰©ä½™ {len(remaining)} ä¸ªåŸå§‹æ–‡ä»¶...")
        for f in remaining:
            f.unlink()
        print(f"   âœ… æ¸…ç†å®Œæˆ")
    
    print(f"\n{'=' * 70}")
    print("â° ç¨‹åºç»“æŸæ—¶é—´:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    print("=" * 70)
    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"ğŸ’¾ æ•°æ®æ ¼å¼: NPY (shape: N Ã— 43)")
    print(f"   åˆ—0: timestamp")
    print(f"   åˆ—1-40: LOB features")
    print(f"   åˆ—41: return_10s")
    print(f"   åˆ—42: return_60s")


if __name__ == "__main__":
    main()

