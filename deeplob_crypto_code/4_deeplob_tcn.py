#!/usr/bin/env python3
"""
DeepLOB-TCN Per-Symbol Training - å¹¶è¡Œç‰ˆæœ¬

TCN Architecture - ç”¨æ—¶åºå·ç§¯ç½‘ç»œ(TCN)æ›¿æ¢LSTMè¿›è¡Œæ—¶åºå»ºæ¨¡

ä¸»è¦æ”¹è¿›:
- ç”¨ TCN æ›¿æ¢ LSTM éƒ¨åˆ†
- ä¿ç•™ DeepLOB çš„ CNN ç‰¹å¾æå–
- TCN æ ¸å¿ƒ: å› æœå·ç§¯ + è†¨èƒ€å·ç§¯ + æ®‹å·®è¿æ¥
- è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œå¹¶è¡Œæ€§æ›´å¥½

æ”¹è¿›ï¼š
1. æ”¯æŒ2ä¸ªæ ‡çš„å¹¶è¡Œè®­ç»ƒ
2. æ—¥å¿—å­˜å‚¨åˆ°logæ–‡ä»¶å¤¹
3. è®­ç»ƒå®Œæˆåç”Ÿæˆ30ä¸ªæ ‡çš„æ±‡æ€»æ—¶åºå›¾
4. ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡æ±‡æ€»è¡¨æ ¼
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import warnings
import time
import json
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import sys
warnings.filterwarnings('ignore')

# è®¾ç½®multiprocessingå¯åŠ¨æ–¹æ³•ä¸º'spawn'ï¼ˆPyTorch CUDAè¦æ±‚ï¼‰
if __name__ == '__main__':
    mp.set_start_method('spawn', force=True)

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# å›ºå®šéšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)


# ============================================================================
# 1. Dataset (ä¸baselineç›¸åŒ)
# ============================================================================

class SingleSymbolLOBDataset(Dataset):
    """å•ä¸ªæ ‡çš„çš„LOBæ•°æ®é›†"""
    
    def __init__(self, file_path, start_ratio=0.0, end_ratio=1.0, 
                 sequence_length=100, scaler=None, target_scaler=None,
                 feature_dim=40, target_col=40, fit_scaler=False):
        
        self.sequence_length = sequence_length
        self.feature_dim = feature_dim
        self.target_col = target_col
        
        data = np.load(file_path, mmap_mode='r')
        n = len(data)
        start_idx = int(n * start_ratio)
        end_idx = int(n * end_ratio)
        segment = data[start_idx:end_idx]
        
        if fit_scaler:
            features = segment[:, :feature_dim]
            targets = segment[:, target_col]
            
            valid_mask = np.isfinite(features).all(axis=1) & np.isfinite(targets)
            features_clean = features[valid_mask]
            targets_clean = targets[valid_mask]
            
            if scaler is None:
                self.scaler = StandardScaler()
                self.scaler.fit(features_clean)
            else:
                self.scaler = scaler
            
            targets_log = np.log1p(targets_clean)
            targets_log = targets_log[np.isfinite(targets_log)]
            
            if target_scaler is None:
                self.target_scaler = StandardScaler()
                self.target_scaler.fit(targets_log.reshape(-1, 1))
            else:
                self.target_scaler = target_scaler
        else:
            self.scaler = scaler
            self.target_scaler = target_scaler
        
        self.data = segment
        self.n_samples = len(self.data) - self.sequence_length
    
    def __len__(self):
        return self.n_samples
    
    def __getitem__(self, idx):
        window = self.data[idx:idx + self.sequence_length, :self.feature_dim].copy()
        target = self.data[idx + self.sequence_length - 1, self.target_col].copy()
        
        if not np.isfinite(window).all() or not np.isfinite(target):
            return torch.zeros(1, self.sequence_length, self.feature_dim), torch.zeros(1)
        
        if self.scaler is not None:
            window = self.scaler.transform(window)
        
        target = np.log1p(target) * 10000  # BPS
        
        if not np.isfinite(target):
            return torch.zeros(1, self.sequence_length, self.feature_dim), torch.zeros(1)
        
        x = torch.FloatTensor(window).unsqueeze(0)
        y = torch.FloatTensor([target])
        
        return x, y


# ============================================================================
# 2. TCN Model (TCN architecture)
# ============================================================================

class TCNBlock(nn.Module):
    """
    TCN æ®‹å·®å— (TCN Residual Block)
    
    æ ¸å¿ƒç»„ä»¶:
    - å› æœå·ç§¯ (Causal Convolution): åªä½¿ç”¨è¿‡å»çš„ä¿¡æ¯
    - è†¨èƒ€å·ç§¯ (Dilated Convolution): æ‰©å¤§æ„Ÿå—é‡
    - æ®‹å·®è¿æ¥ (Residual Connection): ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
    """
    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout=0.2):
        super(TCNBlock, self).__init__()
        
        # å› æœå¡«å……: ç¡®ä¿åªä½¿ç”¨è¿‡å»çš„ä¿¡æ¯ï¼Œé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²
        # padding = (kernel_size - 1) * dilationï¼Œå…¨éƒ¨å¡«å……åœ¨å·¦ä¾§
        self.padding = (kernel_size - 1) * dilation
        
        # ç¬¬ä¸€ä¸ªè†¨èƒ€å·ç§¯å±‚
        self.conv1 = nn.Conv1d(
            in_channels, out_channels, kernel_size,
            padding=0,  # æˆ‘ä»¬æ‰‹åŠ¨è¿›è¡Œå› æœå¡«å……
            dilation=dilation
        )
        self.bn1 = nn.BatchNorm1d(out_channels)
        
        # ç¬¬äºŒä¸ªè†¨èƒ€å·ç§¯å±‚
        self.conv2 = nn.Conv1d(
            out_channels, out_channels, kernel_size,
            padding=0,  # æˆ‘ä»¬æ‰‹åŠ¨è¿›è¡Œå› æœå¡«å……
            dilation=dilation
        )
        self.bn2 = nn.BatchNorm1d(out_channels)
        
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.LeakyReLU(negative_slope=0.01)  # ä½¿ç”¨LeakyReLUé˜²æ­¢Dead ReLU
        
        # æ®‹å·®è¿æ¥: å¦‚æœè¾“å…¥è¾“å‡ºç»´åº¦ä¸åŒï¼Œéœ€è¦1x1å·ç§¯è°ƒæ•´
        self.residual = None
        if in_channels != out_channels:
            self.residual = nn.Conv1d(in_channels, out_channels, 1)
        
        # æƒé‡åˆå§‹åŒ–
        nn.init.kaiming_normal_(self.conv1.weight)
        nn.init.kaiming_normal_(self.conv2.weight)
        if self.residual is not None:
            nn.init.kaiming_normal_(self.residual.weight)
    
    def forward(self, x):
        """
        Args:
            x: (batch, channels, seq_len)
        Returns:
            out: (batch, channels, seq_len)
        """
        residual = x
        
        # ç¬¬ä¸€ä¸ªå·ç§¯ + å› æœå¡«å……
        # å·¦ä¾§å¡«å……ï¼Œå³ä¾§ä¸å¡«å……ï¼Œç¡®ä¿å› æœæ€§
        out = F.pad(x, (self.padding, 0))
        out = self.conv1(out)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.dropout(out)
        
        # ç¬¬äºŒä¸ªå·ç§¯ + å› æœå¡«å……
        out = F.pad(out, (self.padding, 0))
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.dropout(out)
        
        # æ®‹å·®è¿æ¥
        if self.residual is not None:
            residual = self.residual(residual)
        
        out += residual
        out = self.relu(out)
        
        return out


class TCN(nn.Module):
    """
    æ—¶åºå·ç§¯ç½‘ç»œ (Temporal Convolutional Network)
    
    ç‰¹ç‚¹:
    - å¤šå±‚ TCN å—å †å 
    - æŒ‡æ•°å¢é•¿çš„è†¨èƒ€ç‡ (1, 2, 4, 8, ...)
    - æ„Ÿå—é‡éšå±‚æ•°æŒ‡æ•°å¢é•¿
    - å®Œå…¨å¹¶è¡Œå¤„ç†ï¼Œè®­ç»ƒé€Ÿåº¦å¿«
    """
    def __init__(self, input_size, num_channels, kernel_size=3, dropout=0.2):
        """
        Args:
            input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
            num_channels: æ¯å±‚çš„é€šé“æ•°åˆ—è¡¨ï¼Œå¦‚ [64, 64, 64]
            kernel_size: å·ç§¯æ ¸å¤§å°
            dropout: Dropout æ¯”ç‡
        """
        super(TCN, self).__init__()
        
        layers = []
        num_levels = len(num_channels)
        
        for i in range(num_levels):
            dilation = 2 ** i  # æŒ‡æ•°å¢é•¿çš„è†¨èƒ€ç‡: 1, 2, 4, 8, ...
            in_channels = input_size if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            
            layers.append(
                TCNBlock(in_channels, out_channels, kernel_size, 
                        dilation, dropout)
            )
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        """
        Args:
            x: (batch, channels, seq_len)
        Returns:
            out: (batch, channels, seq_len)
        """
        return self.network(x)


class DeepLOB_TCN(nn.Module):
    """
    DeepLOB-TCN æ¨¡å‹æ¶æ„ (with Batch Normalization)
    
    æ¶æ„æµç¨‹:
    1. CNN ç‰¹å¾æå– (ä¿ç•™ DeepLOB çš„ CNN éƒ¨åˆ† + BN)
       - 3ä¸ªå·ç§¯å— + BN
       - Inception æ¨¡å— + BN
    2. TCN æ—¶åºå»ºæ¨¡ (æ›¿æ¢ LSTM)
       - å¤šå±‚ TCN å—
       - å› æœå·ç§¯ + è†¨èƒ€å·ç§¯ + æ®‹å·®è¿æ¥
    3. å…¨è¿æ¥å±‚é¢„æµ‹ + BN
    
    ä¼˜åŠ¿:
    - ä¿ç•™äº† DeepLOB çš„ CNN ç‰¹å¾æå–èƒ½åŠ›
    - ç”¨ TCN æ›¿æ¢ LSTMï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«
    - æ›´å¥½çš„å¹¶è¡Œæ€§å’Œé•¿è·ç¦»ä¾èµ–å»ºæ¨¡
    - BNå±‚é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼ŒåŠ é€Ÿæ”¶æ•›
    """
    def __init__(self, input_channels=1, num_classes=1, dropout=0.3):
        super(DeepLOB_TCN, self).__init__()
        
        # ==================== CNN ç‰¹å¾æå–éƒ¨åˆ† + BN ====================
        # First convolutional block
        self.conv1a = nn.Conv2d(input_channels, 32, kernel_size=(1, 2), stride=(1, 2))
        self.bn1a = nn.BatchNorm2d(32)
        self.conv1b = nn.Conv2d(32, 32, kernel_size=(4, 1), padding=(0, 0))
        self.bn1b = nn.BatchNorm2d(32)
        self.conv1c = nn.Conv2d(32, 32, kernel_size=(4, 1), padding=(0, 0))
        self.bn1c = nn.BatchNorm2d(32)
        
        # Second convolutional block
        self.conv2a = nn.Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 2))
        self.bn2a = nn.BatchNorm2d(32)
        self.conv2b = nn.Conv2d(32, 32, kernel_size=(4, 1), padding=(0, 0))
        self.bn2b = nn.BatchNorm2d(32)
        self.conv2c = nn.Conv2d(32, 32, kernel_size=(4, 1), padding=(0, 0))
        self.bn2c = nn.BatchNorm2d(32)
        
        # Third convolutional block
        self.conv3a = nn.Conv2d(32, 32, kernel_size=(1, 10))
        self.bn3a = nn.BatchNorm2d(32)
        
        # Inception module
        self.inception1 = nn.Conv2d(32, 64, kernel_size=(1, 1), stride=1)
        self.bn_inc1 = nn.BatchNorm2d(64)
        
        self.inception2a = nn.Conv2d(32, 64, kernel_size=(1, 1), stride=1)
        self.inception2b = nn.Conv2d(64, 64, kernel_size=(3, 1), stride=1, padding=(1, 0))
        self.bn_inc2 = nn.BatchNorm2d(64)
        
        self.inception3a = nn.Conv2d(32, 64, kernel_size=(1, 1), stride=1)
        self.inception3b = nn.Conv2d(64, 64, kernel_size=(5, 1), stride=1, padding=(2, 0))
        self.bn_inc3 = nn.BatchNorm2d(64)
        
        self.inception4 = nn.MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
        self.inception4_conv = nn.Conv2d(32, 64, kernel_size=(1, 1), stride=1)
        self.bn_inc4 = nn.BatchNorm2d(64)
        
        # ==================== TCN æ—¶åºå»ºæ¨¡éƒ¨åˆ† (æ›¿æ¢ LSTM) ====================
        # è¾“å…¥ç»´åº¦: 256 (Inception è¾“å‡º: 64*4=256)
        # TCN é…ç½®: 4å±‚ï¼Œæ¯å±‚64é€šé“
        # è†¨èƒ€ç‡: 1, 2, 4, 8
        self.tcn = TCN(
            input_size=256,
            num_channels=[64, 64, 64, 64],  # 4å±‚TCNï¼Œæ¯å±‚64é€šé“
            kernel_size=3,
            dropout=dropout
        )
        
        # ==================== å…¨è¿æ¥å±‚ + BN ====================
        # TCN è¾“å‡º: 64 é€šé“
        self.fc1 = nn.Linear(64, 64)
        self.bn_fc1 = nn.BatchNorm1d(64)
        self.fc2 = nn.Linear(64, num_classes)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # ==================== CNN ç‰¹å¾æå– + BN + LeakyReLU ====================
        # Input: (batch, 1, seq_len, features)
        x = F.leaky_relu(self.bn1a(self.conv1a(x)), negative_slope=0.01)
        x = F.leaky_relu(self.bn1b(self.conv1b(x)), negative_slope=0.01)
        x = F.leaky_relu(self.bn1c(self.conv1c(x)), negative_slope=0.01)
        
        x = F.leaky_relu(self.bn2a(self.conv2a(x)), negative_slope=0.01)
        x = F.leaky_relu(self.bn2b(self.conv2b(x)), negative_slope=0.01)
        x = F.leaky_relu(self.bn2c(self.conv2c(x)), negative_slope=0.01)
        
        x = F.leaky_relu(self.bn3a(self.conv3a(x)), negative_slope=0.01)
        
        # Inception module with BN + LeakyReLU
        branch1 = F.leaky_relu(self.bn_inc1(self.inception1(x)), negative_slope=0.01)
        
        branch2 = F.leaky_relu(self.inception2a(x), negative_slope=0.01)
        branch2 = F.leaky_relu(self.bn_inc2(self.inception2b(branch2)), negative_slope=0.01)
        
        branch3 = F.leaky_relu(self.inception3a(x), negative_slope=0.01)
        branch3 = F.leaky_relu(self.bn_inc3(self.inception3b(branch3)), negative_slope=0.01)
        
        branch4 = self.inception4(x)
        branch4 = F.leaky_relu(self.bn_inc4(self.inception4_conv(branch4)), negative_slope=0.01)
        
        x = torch.cat([branch1, branch2, branch3, branch4], dim=1)
        # Output: (batch, 256, seq_len, 1)
        
        # Reshape for TCN: (batch, 256, seq_len)
        x = x.squeeze(-1).permute(0, 2, 1).permute(0, 2, 1)
        
        # ==================== TCN æ—¶åºå»ºæ¨¡ (æ›¿æ¢ LSTM) ====================
        x = self.tcn(x)  # (batch, 64, seq_len)
        
        # å–æœ€åæ—¶é—´æ­¥çš„è¾“å‡º
        x = x[:, :, -1]  # (batch, 64)
        
        # ==================== å…¨è¿æ¥å±‚ + BN + LeakyReLU ====================
        x = F.leaky_relu(self.bn_fc1(self.fc1(x)), negative_slope=0.01)
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x


# ============================================================================
# 3. Training Function (å•è¿›ç¨‹ç‰ˆæœ¬ï¼Œç”¨äºå¹¶è¡Œè°ƒç”¨)
# ============================================================================

def train_single_symbol_worker(args):
    """å•æ ‡çš„è®­ç»ƒå‡½æ•°ï¼ˆç”¨äºå¹¶è¡Œè°ƒç”¨ï¼‰"""
    symbol, data_dir, output_dir, log_dir, config, gpu_id = args
    
    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„GPU
    import os
    # å¯¹äºå•GPUå¹¶è¡Œï¼Œä¸¤ä¸ªè¿›ç¨‹éƒ½ä½¿ç”¨GPU 0
    # å¯¹äºå¤šGPUï¼Œæ¯ä¸ªè¿›ç¨‹ä½¿ç”¨ä¸åŒçš„GPU
    if gpu_id >= torch.cuda.device_count():
        gpu_id = 0  # å¦‚æœGPU IDè¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨GPU 0
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')
    
    # è®¾ç½®æ—¥å¿—æ–‡ä»¶
    log_file = log_dir / f"{symbol}_training.log"
    log_f = open(log_file, 'w')
    
    def log_print(*args, **kwargs):
        msg = ' '.join(str(a) for a in args)
        print(msg)
        log_f.write(msg + '\n')
        log_f.flush()
    
    try:
        log_print(f"\n{'='*80}")
        log_print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {symbol} (GPU {gpu_id}) - DeepLOB-TCN")
        log_print(f"{'='*80}")
        
        start_time = time.time()
        
        # æ–‡ä»¶è·¯å¾„
        data_file = Path(data_dir) / f"{symbol}_20250801_20250810.npy"
        if not data_file.exists():
            log_print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return None
        
        # åˆ›å»ºæ•°æ®é›†
        log_print(f"   ğŸ“‚ åŠ è½½æ•°æ®...")
        train_dataset = SingleSymbolLOBDataset(
            data_file, start_ratio=0.0, end_ratio=0.6,
            sequence_length=config['sequence_length'], fit_scaler=True
        )
        
        val_dataset = SingleSymbolLOBDataset(
            data_file, start_ratio=0.6, end_ratio=0.8,
            sequence_length=config['sequence_length'],
            scaler=train_dataset.scaler,
            target_scaler=train_dataset.target_scaler,
            fit_scaler=False
        )
        
        test_dataset = SingleSymbolLOBDataset(
            data_file, start_ratio=0.8, end_ratio=1.0,
            sequence_length=config['sequence_length'],
            scaler=train_dataset.scaler,
            target_scaler=train_dataset.target_scaler,
            fit_scaler=False
        )
        
        log_print(f"      Train: {len(train_dataset):,} samples")
        log_print(f"      Val:   {len(val_dataset):,} samples")
        log_print(f"      Test:  {len(test_dataset):,} samples")
        
        # DataLoaders
        train_loader = DataLoader(
            train_dataset, batch_size=config['batch_size'],
            shuffle=True, num_workers=config['num_workers'], pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset, batch_size=config['batch_size'],
            shuffle=False, num_workers=config['num_workers'], pin_memory=True
        )
        
        test_loader = DataLoader(
            test_dataset, batch_size=config['batch_size'],
            shuffle=False, num_workers=config['num_workers'], pin_memory=True
        )
        
        # åˆ›å»ºæ¨¡å‹ - ä½¿ç”¨TCNæ¶æ„
        log_print(f"   ğŸ—ï¸  åˆ›å»ºæ¨¡å‹... (DeepLOB-TCN)")
        model = DeepLOB_TCN(
            input_channels=1, num_classes=1, dropout=config['dropout']
        ).to(device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])
        criterion = nn.HuberLoss(delta=1.0)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=3
        )
        
        # è®­ç»ƒå¾ªç¯
        log_print(f"   ğŸ‹ï¸  å¼€å§‹è®­ç»ƒ...")
        best_val_loss = float('inf')
        patience_counter = 0
        epoch_times = []
        history = {
            'train_loss': [],
            'val_loss': [],
            'lr': [],
            'epoch_times': []
        }
        
        for epoch in range(config['num_epochs']):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            model.train()
            train_loss = 0.0
            for batch_x, batch_y in train_loader:
                batch_x = batch_x.to(device)
                batch_y = batch_y.to(device)
                
                optimizer.zero_grad()
                outputs = model(batch_x)
                loss = criterion(outputs.squeeze(), batch_y.squeeze())
                loss.backward()
                
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # éªŒè¯
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for batch_x, batch_y in val_loader:
                    batch_x = batch_x.to(device)
                    batch_y = batch_y.to(device)
                    
                    outputs = model(batch_x)
                    loss = criterion(outputs.squeeze(), batch_y.squeeze())
                    val_loss += loss.item()
            
            val_loss /= len(val_loader)
            
            scheduler.step(val_loss)
            current_lr = optimizer.param_groups[0]['lr']
            
            epoch_time = time.time() - epoch_start
            epoch_times.append(epoch_time)
            
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['lr'].append(current_lr)
            history['epoch_times'].append(epoch_time)
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'scaler': train_dataset.scaler,
                    'target_scaler': train_dataset.target_scaler,
                    'config': config
                }, output_dir / f"{symbol}_best_model.pth")
            else:
                patience_counter += 1
            
            if (epoch + 1) % 5 == 0 or epoch == 0:
                log_print(f"      Epoch {epoch+1:2d}/{config['num_epochs']} | "
                          f"Train: {train_loss:.6f} | Val: {val_loss:.6f} | "
                          f"LR: {current_lr:.6f} | Time: {epoch_time:.2f}s")
            
            if patience_counter >= config['early_stopping_patience']:
                log_print(f"      â¹ï¸  Early stopping at epoch {epoch+1}")
                break
        
        if epoch_times:
            avg_epoch_time = np.mean(epoch_times)
            log_print(f"      â±ï¸  å¹³å‡æ¯ä¸ªEpoch: {avg_epoch_time:.2f}ç§’")
        
        # æµ‹è¯•
        log_print(f"   ğŸ“Š æµ‹è¯•æœ€ä½³æ¨¡å‹...")
        checkpoint = torch.load(output_dir / f"{symbol}_best_model.pth", weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        test_preds = []
        test_targets = []
        test_loss = 0.0
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(device)
                batch_y = batch_y.to(device)
                
                outputs = model(batch_x)
                loss = criterion(outputs.squeeze(), batch_y.squeeze())
                test_loss += loss.item()
                
                test_preds.append(outputs.cpu().numpy())
                test_targets.append(batch_y.cpu().numpy())
        
        test_loss /= len(test_loader)
        test_preds = np.concatenate(test_preds).flatten()
        test_targets = np.concatenate(test_targets).flatten()
        
        # è®¡ç®—æŒ‡æ ‡
        mae = mean_absolute_error(test_targets, test_preds)
        rmse = np.sqrt(mean_squared_error(test_targets, test_preds))
        r2 = r2_score(test_targets, test_preds)
        corr = np.corrcoef(test_targets, test_preds)[0, 1] if len(test_targets) > 1 else 0.0
        
        training_time = time.time() - start_time
        
        log_print(f"\n   âœ… è®­ç»ƒå®Œæˆ!")
        log_print(f"      Test Loss: {test_loss:.6f}")
        log_print(f"      MAE:       {mae:.6f}")
        log_print(f"      RMSE:      {rmse:.6f}")
        log_print(f"      RÂ²:        {r2:.6f}")
        log_print(f"      Corr:      {corr:.6f}")
        log_print(f"      Time:      {training_time/60:.2f} min")
        
        # ä¿å­˜ç»“æœ
        result = {
            'symbol': symbol,
            'test_loss': float(test_loss),
            'mae': float(mae),
            'rmse': float(rmse),
            'r2': float(r2),
            'correlation': float(corr),
            'best_val_loss': float(best_val_loss),
            'training_time_minutes': float(training_time / 60),
            'epochs_trained': len(history['train_loss']),
            'train_samples': len(train_dataset),
            'val_samples': len(val_dataset),
            'test_samples': len(test_dataset)
        }
        
        # ä¿å­˜å†å²
        with open(output_dir / f"{symbol}_history.pkl", 'wb') as f:
            pickle.dump(history, f)
        
        # ä¿å­˜é¢„æµ‹
        np.savez(
            output_dir / f"{symbol}_predictions.npz",
            predictions=test_preds,
            targets=test_targets
        )
        
        log_f.close()
        return result
    
    except Exception as e:
        log_print(f"   âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        log_print(traceback.format_exc())
        log_f.close()
        return None


# ============================================================================
# 4. Plotting Functions
# ============================================================================

def plot_timeseries_comparison(y_true, y_pred, symbol, output_dir):
    """ç»˜åˆ¶å•ä¸ªæ ‡çš„çš„æ—¶åºå¯¹æ¯”å›¾"""
    window_size = min(1000, len(y_true))
    min_zero_ratio = 1.0
    best_start = 0
    
    for i in range(len(y_true) - window_size + 1):
        window = y_true[i:i+window_size]
        zero_ratio = np.sum(np.abs(window) < 0.01) / window_size
        if zero_ratio < min_zero_ratio:
            min_zero_ratio = zero_ratio
            best_start = i
    
    if min_zero_ratio > 0.5:
        non_zero_mask = np.abs(y_true) > 0.01
        non_zero_indices = np.where(non_zero_mask)[0]
        
        if len(non_zero_indices) >= window_size:
            np.random.seed(42)
            selected_indices = np.random.choice(non_zero_indices, window_size, replace=False)
            selected_indices = np.sort(selected_indices)
            y_true_plot = y_true[selected_indices]
            y_pred_plot = y_pred[selected_indices]
            indices_plot = np.arange(len(y_true_plot))
        else:
            y_true_plot = y_true[non_zero_indices]
            y_pred_plot = y_pred[non_zero_indices]
            indices_plot = np.arange(len(y_true_plot))
    else:
        y_true_plot = y_true[best_start:best_start+window_size]
        y_pred_plot = y_pred[best_start:best_start+window_size]
        indices_plot = np.arange(len(y_true_plot))
    
    fig, ax = plt.subplots(1, 1, figsize=(14, 6))
    ax.plot(indices_plot, y_true_plot, color='#2E86AB', linewidth=1.5, 
           label='True', alpha=0.8, marker='o', markersize=2, markevery=max(1, len(indices_plot)//20))
    ax.plot(indices_plot, y_pred_plot, color='#E63946', linewidth=1.5, 
           label='Pred', alpha=0.8, marker='s', markersize=2, markevery=max(1, len(indices_plot)//20))
    ax.axhline(0, color='black', linestyle=':', linewidth=0.5, alpha=0.5)
    ax.set_xlabel('Index', fontsize=10)
    ax.set_ylabel('Return (BPS)', fontsize=10)
    ax.set_title(f'{symbol}', fontsize=11, fontweight='bold')
    ax.legend(loc='upper right', fontsize=9)
    ax.grid(True, alpha=0.2, linestyle='--')
    
    corr = np.corrcoef(y_true, y_pred)[0, 1]
    ax.text(0.02, 0.98, f'Corr: {corr:.3f}', transform=ax.transAxes,
           fontsize=9, verticalalignment='top',
           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))
    
    plt.tight_layout()
    plt.savefig(output_dir / f"{symbol}_timeseries.png", dpi=200, bbox_inches='tight', facecolor='white')
    plt.close()


def plot_all_symbols_timeseries(output_dir, symbols, image_dir):
    """ç»˜åˆ¶æ‰€æœ‰30ä¸ªæ ‡çš„çš„æ±‡æ€»æ—¶åºå›¾"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ç”Ÿæˆ30ä¸ªæ ‡çš„æ±‡æ€»æ—¶åºå›¾ (TCN)")
    print(f"{'='*80}")
    
    # è®¡ç®—å¸ƒå±€ï¼š6è¡Œ5åˆ—
    n_symbols = len(symbols)
    n_rows = 6
    n_cols = 5
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 24))
    axes = axes.flatten()
    
    plot_count = 0
    
    for i, symbol in enumerate(symbols):
        pred_file = output_dir / f"{symbol}_predictions.npz"
        
        if not pred_file.exists():
            axes[i].axis('off')
            axes[i].text(0.5, 0.5, f'{symbol}\nNo Data', 
                        ha='center', va='center', fontsize=10)
            continue
        
        try:
            data = np.load(pred_file)
            y_true = data['targets']
            y_pred = data['predictions']
            
            # é€‰æ‹©ç»˜å›¾åŒºåŸŸï¼ˆé›¶å€¼è¾ƒå°‘çš„åŒºåŸŸï¼‰
            window_size = min(500, len(y_true))  # æ¯ä¸ªæ ‡çš„åªæ˜¾ç¤º500ä¸ªæ ·æœ¬
            min_zero_ratio = 1.0
            best_start = 0
            
            for j in range(len(y_true) - window_size + 1):
                window = y_true[j:j+window_size]
                zero_ratio = np.sum(np.abs(window) < 0.01) / window_size
                if zero_ratio < min_zero_ratio:
                    min_zero_ratio = zero_ratio
                    best_start = j
            
            if min_zero_ratio > 0.5:
                non_zero_mask = np.abs(y_true) > 0.01
                non_zero_indices = np.where(non_zero_mask)[0]
                if len(non_zero_indices) >= window_size:
                    np.random.seed(42)
                    selected_indices = np.random.choice(non_zero_indices, window_size, replace=False)
                    selected_indices = np.sort(selected_indices)
                    y_true_plot = y_true[selected_indices]
                    y_pred_plot = y_pred[selected_indices]
                    indices_plot = np.arange(len(y_true_plot))
                else:
                    y_true_plot = y_true[non_zero_indices]
                    y_pred_plot = y_pred[non_zero_indices]
                    indices_plot = np.arange(len(y_true_plot))
            else:
                y_true_plot = y_true[best_start:best_start+window_size]
                y_pred_plot = y_pred[best_start:best_start+window_size]
                indices_plot = np.arange(len(y_true_plot))
            
            # ç»˜åˆ¶
            ax = axes[i]
            ax.plot(indices_plot, y_true_plot, color='#2E86AB', linewidth=1.0, 
                   label='True', alpha=0.7, markersize=1)
            ax.plot(indices_plot, y_pred_plot, color='#E63946', linewidth=1.0, 
                   label='Pred', alpha=0.7, markersize=1)
            ax.axhline(0, color='black', linestyle=':', linewidth=0.5, alpha=0.3)
            
            # è®¡ç®—ç›¸å…³æ€§
            corr = np.corrcoef(y_true, y_pred)[0, 1] if len(y_true) > 1 else 0.0
            
            ax.set_title(f'{symbol} (Corr: {corr:.3f})', fontsize=9, fontweight='bold', pad=3)
            ax.tick_params(labelsize=7)
            ax.grid(True, alpha=0.2, linestyle='--', linewidth=0.5)
            ax.legend(loc='upper right', fontsize=7, framealpha=0.8)
            
            plot_count += 1
            
        except Exception as e:
            axes[i].axis('off')
            axes[i].text(0.5, 0.5, f'{symbol}\nError: {str(e)[:20]}', 
                        ha='center', va='center', fontsize=8)
    
    # éšè—å¤šä½™çš„å­å›¾
    for i in range(n_symbols, len(axes)):
        axes[i].axis('off')
    
    plt.suptitle('Time Series Comparison - All 30 Symbols (TCN)', 
                fontsize=16, fontweight='bold', y=0.995)
    plt.tight_layout(rect=[0, 0, 1, 0.99])
    
    save_path = image_dir / 'all_symbols_timeseries_summary_tcn.png'
    plt.savefig(save_path, dpi=200, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"   âœ… æ±‡æ€»æ—¶åºå›¾å·²ä¿å­˜: {save_path}")
    print(f"      æˆåŠŸç»˜åˆ¶: {plot_count}/{n_symbols} ä¸ªæ ‡çš„")


# ============================================================================
# 5. Main Function
# ============================================================================

def main():
    print("="*80)
    print("ğŸš€ DeepLOB-TCN Per-Symbol Training (Parallel)")
    print("="*80)
    print("\nç­–ç•¥: æ¯ä¸ªæ ‡çš„ç‹¬ç«‹è®­ç»ƒï¼Œæ”¯æŒå¤šGPUå¹¶è¡Œ")
    print("æ•°æ®: 10å¤©æ•°æ® (2025-08-01 to 2025-08-10)")
    print("æ¨¡å‹: DeepLOB-TCN with BatchNorm + LeakyReLU + HuberLoss")
    
    # é…ç½®
    config = {
        'sequence_length': 100,
        'batch_size': 2048,
        'num_workers': 2,
        'learning_rate': 0.001,
        'num_epochs': 20,
        'dropout': 0.3,
        'early_stopping_patience': 5
    }
    
    # è·¯å¾„ - æ‰€æœ‰è¾“å‡ºä¿å­˜åˆ°5_modelsæ–‡ä»¶å¤¹
    data_dir = Path('data_250801_250810')
    output_dir = Path('5_models')
    log_dir = output_dir / 'log'
    image_dir = output_dir / 'image'
    
    output_dir.mkdir(exist_ok=True)
    log_dir.mkdir(exist_ok=True)
    image_dir.mkdir(exist_ok=True)
    
    # è¯»å–æ‰€æœ‰æ ‡çš„
    metadata_file = data_dir / 'metadata.csv'
    if metadata_file.exists():
        metadata = pd.read_csv(metadata_file)
        symbols = metadata['symbol'].tolist()
    else:
        npy_files = sorted(data_dir.glob('*_20250801_20250810.npy'))
        symbols = [f.stem.replace('_20250801_20250810', '') for f in npy_files]
    
    print(f"\næ‰¾åˆ° {len(symbols)} ä¸ªæ ‡çš„")
    print(f"é…ç½®: {json.dumps(config, indent=2)}")
    print(f"æ—¥å¿—ç›®å½•: {log_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # âœ… æ–­ç‚¹æ¢å¤ï¼šæ£€æŸ¥å·²å®Œæˆçš„æ ‡çš„
    completed_symbols = []
    pending_symbols = []
    for symbol in symbols:
        model_file = output_dir / f"{symbol}_best_model.pth"
        if model_file.exists():
            completed_symbols.append(symbol)
        else:
            pending_symbols.append(symbol)
    
    print(f"\næ–­ç‚¹æ¢å¤:")
    print(f"   âœ… å·²å®Œæˆ: {len(completed_symbols)} ä¸ª")
    print(f"   â³ å¾…è®­ç»ƒ: {len(pending_symbols)} ä¸ª")
    
    if completed_symbols:
        print(f"\nå·²å®Œæˆçš„æ ‡çš„: {', '.join(completed_symbols[:10])}" +
              (f"... (å…±{len(completed_symbols)}ä¸ª)" if len(completed_symbols) > 10 else ""))
    
    if pending_symbols:
        print(f"\nå¾…è®­ç»ƒçš„æ ‡çš„: {', '.join(pending_symbols)}")
        symbols = pending_symbols  # åªè®­ç»ƒæœªå®Œæˆçš„æ ‡çš„
    else:
        print("\nâœ… æ‰€æœ‰æ ‡çš„å·²è®­ç»ƒå®Œæˆï¼")
        symbols = []  # ä¸éœ€è¦è®­ç»ƒï¼Œç›´æ¥ç”Ÿæˆæ±‡æ€»
    
    # å‡†å¤‡å¹¶è¡Œè®­ç»ƒå‚æ•°
    n_gpus = torch.cuda.device_count()
    print(f"\nå¯ç”¨GPUæ•°é‡: {n_gpus}")
    
    # ğŸš€ æ”¯æŒæ¯å¼ GPUå¹¶è¡Œ2ä¸ªæ¨¡å‹
    if n_gpus >= 4:
        max_workers = 8  # 4å¼ GPU Ã— 2ä¸ªè¿›ç¨‹/GPU = 8ä¸ªå¹¶è¡Œ
        print(f"âœ… å°†ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œè®­ç»ƒ (æ¯å¼ GPU 2ä¸ªè¿›ç¨‹)")
    elif n_gpus >= 2:
        max_workers = n_gpus * 2  # æ¯å¼ GPU 2ä¸ªè¿›ç¨‹
        print(f"âœ… å°†ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œè®­ç»ƒ (æ¯å¼ GPU 2ä¸ªè¿›ç¨‹)")
    else:
        max_workers = 2  # å•GPUæ—¶ä½¿ç”¨2ä¸ªè¿›ç¨‹å…±äº«
        print(f"âœ… å°†ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å…±äº«1ä¸ªGPUå¹¶è¡Œè®­ç»ƒ")
    
    print(f"   Batch Size: {config['batch_size']}")
    print(f"   é¢„è®¡æ¯å¼ GPUæ˜¾å­˜ä½¿ç”¨: ~14GB Ã— 2 = ~28GB (å®‰å…¨èŒƒå›´å†…)")
    
    # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨ - æ¯å¼ GPUåˆ†é…2ä¸ªä»»åŠ¡
    tasks = []
    for i, symbol in enumerate(symbols):
        gpu_id = (i // 2) % n_gpus if n_gpus > 0 else 0  # æ¯2ä¸ªä»»åŠ¡åˆ†é…åˆ°ä¸€ä¸ªGPU
        tasks.append((symbol, data_dir, output_dir, log_dir, config, gpu_id))
    
    results = []
    failed_symbols = []
    
    if tasks:
        # å¹¶è¡Œè®­ç»ƒ
        print(f"\n{'='*80}")
        print(f"ğŸ‹ï¸  å¼€å§‹å¹¶è¡Œè®­ç»ƒ ({max_workers} ä¸ªè¿›ç¨‹)")
        print(f"{'='*80}\n")
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            future_to_symbol = {executor.submit(train_single_symbol_worker, task): task[0] 
                               for task in tasks}
            
            completed = 0
            for future in as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                completed += 1
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                        print(f"[{completed}/{len(tasks)}] âœ… {symbol} å®Œæˆ")
                    else:
                        failed_symbols.append(symbol)
                        print(f"[{completed}/{len(tasks)}] âŒ {symbol} å¤±è´¥")
                except Exception as e:
                    failed_symbols.append(symbol)
                    print(f"[{completed}/{len(tasks)}] âŒ {symbol} å¼‚å¸¸: {e}")
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    print(f"\n{'='*80}")
    print(f"ğŸ“Š è®­ç»ƒæ±‡æ€» - åŠ è½½æ‰€æœ‰å·²å®Œæˆçš„æ ‡çš„")
    print(f"{'='*80}")
    
    # è¯»å–æ‰€æœ‰æ ‡çš„ï¼ˆåŒ…æ‹¬ä¹‹å‰å®Œæˆçš„å’Œåˆšå®Œæˆçš„ï¼‰
    metadata = pd.read_csv(metadata_file)
    all_symbols_list = metadata['symbol'].tolist()
    
    all_results = []
    for symbol in all_symbols_list:
        pred_file = output_dir / f"{symbol}_predictions.npz"
        history_file = output_dir / f"{symbol}_history.pkl"
        
        if pred_file.exists():
            try:
                data = np.load(pred_file)
                y_true = data['targets']
                y_pred = data['predictions']
                
                mae = mean_absolute_error(y_true, y_pred)
                rmse = np.sqrt(mean_squared_error(y_true, y_pred))
                r2 = r2_score(y_true, y_pred)
                corr = np.corrcoef(y_true, y_pred)[0, 1]
                
                # å°è¯•ä»å†å²æ–‡ä»¶è¯»å–è®­ç»ƒä¿¡æ¯
                epochs_trained = 0
                training_time_minutes = 0.0
                best_val_loss = 0.0
                
                if history_file.exists():
                    try:
                        with open(history_file, 'rb') as f:
                            history = pickle.load(f)
                            epochs_trained = len(history.get('train_loss', []))
                            if 'epoch_times' in history and history['epoch_times']:
                                training_time_minutes = sum(history['epoch_times']) / 60.0
                            if 'val_loss' in history and history['val_loss']:
                                best_val_loss = min(history['val_loss'])
                    except:
                        pass
                
                all_results.append({
                    'symbol': symbol,
                    'test_loss': 0.0,  # å ä½ç¬¦
                    'mae': float(mae),
                    'rmse': float(rmse),
                    'r2': float(r2),
                    'correlation': float(corr),
                    'best_val_loss': float(best_val_loss),
                    'training_time_minutes': float(training_time_minutes),
                    'epochs_trained': int(epochs_trained),
                    'train_samples': len(y_true),
                    'val_samples': 0,
                    'test_samples': len(y_true)
                })
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ {symbol} æ—¶å‡ºé”™: {e}")
    
    if all_results:
        df_results = pd.DataFrame(all_results)
        df_results = df_results.sort_values('correlation', ascending=False)
        
        # ä¿å­˜CSV
        csv_file = output_dir / 'training_summary_tcn.csv'
        df_results.to_csv(csv_file, index=False)
        print(f"\nâœ… æ±‡æ€»è¡¨æ ¼å·²ä¿å­˜: {csv_file}")
        
        # ç”ŸæˆMarkdownè¡¨æ ¼
        md_file = output_dir / 'training_summary_tcn.md'
        with open(md_file, 'w') as f:
            f.write("# DeepLOB-TCN Per-Symbol Training Summary\n\n")
            f.write(f"**è®­ç»ƒæ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**æ•°æ®**: 10å¤©æ•°æ® (2025-08-01 to 2025-08-10)\n\n")
            f.write(f"**æˆåŠŸè®­ç»ƒ**: {len(all_results)}/{len(all_symbols_list)} ä¸ªæ ‡çš„\n\n")
            
            f.write("## æ€§èƒ½æŒ‡æ ‡æ±‡æ€»\n\n")
            f.write("| æ’å | æ ‡çš„ | MAE | RMSE | RÂ² | Correlation | è®­ç»ƒæ—¶é—´(åˆ†é’Ÿ) | Epochs |\n")
            f.write("|------|------|-----|------|----|-------------|----------------|--------|\n")
            
            for idx, row in df_results.iterrows():
                f.write(f"| {idx+1} | {row['symbol']} | {row['mae']:.6f} | {row['rmse']:.6f} | "
                       f"{row['r2']:.6f} | **{row['correlation']:.6f}** | "
                       f"{row['training_time_minutes']:.2f} | {row['epochs_trained']} |\n")
            
            f.write("\n## ç»Ÿè®¡æ‘˜è¦\n\n")
            f.write(f"- **å¹³å‡ MAE**: {df_results['mae'].mean():.6f}\n")
            f.write(f"- **å¹³å‡ RMSE**: {df_results['rmse'].mean():.6f}\n")
            f.write(f"- **å¹³å‡ RÂ²**: {df_results['r2'].mean():.6f}\n")
            f.write(f"- **å¹³å‡ Correlation**: {df_results['correlation'].mean():.6f}\n")
            f.write(f"- **æ€»è®­ç»ƒæ—¶é—´**: {df_results['training_time_minutes'].sum():.2f} åˆ†é’Ÿ\n")
            f.write(f"- **å¹³å‡è®­ç»ƒæ—¶é—´**: {df_results['training_time_minutes'].mean():.2f} åˆ†é’Ÿ\n")
        
        print(f"âœ… Markdownè¡¨æ ¼å·²ä¿å­˜: {md_file}")
        
        # æ‰“å°ç»Ÿè®¡
        print(f"\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
        print(f"   æˆåŠŸè®­ç»ƒ: {len(all_results)}/{len(all_symbols_list)}")
        print(f"   å¹³å‡ MAE:  {df_results['mae'].mean():.6f}")
        print(f"   å¹³å‡ Corr: {df_results['correlation'].mean():.6f}")
        print(f"   å¹³å‡ RÂ²:   {df_results['r2'].mean():.6f}")
    
        print(f"\nğŸ† Top 10 æ ‡çš„ (æŒ‰Correlationæ’åº):")
        print(df_results[['symbol', 'correlation', 'mae', 'r2']].head(10).to_string(index=False))
        
        # æ‰“å°å®Œæ•´ç»“æœè¡¨æ ¼
        print(f"\n{'='*80}")
        print(f"ğŸ“Š å®Œæ•´è®­ç»ƒç»“æœè¡¨æ ¼")
        print(f"{'='*80}\n")
        
        # æ ¼å¼åŒ–è¾“å‡ºæ‰€æœ‰ç»“æœ
        print(f"{'æ’å':<6}{'æ ‡çš„':<15}{'MAE':<12}{'RMSE':<12}{'RÂ²':<12}{'Correlation':<15}{'è®­ç»ƒæ—¶é—´':<12}")
        print(f"{'-'*85}")
        for idx, row in df_results.iterrows():
            print(f"{idx+1:<6}{row['symbol']:<15}{row['mae']:<12.6f}{row['rmse']:<12.6f}"
                  f"{row['r2']:<12.6f}{row['correlation']:<15.6f}{row['training_time_minutes']:<12.2f}")
        
        print(f"\n{'='*80}")
        print(f"ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦")
        print(f"{'='*80}")
        print(f"  å¹³å‡ MAE:           {df_results['mae'].mean():.6f}")
        print(f"  å¹³å‡ RMSE:          {df_results['rmse'].mean():.6f}")
        print(f"  å¹³å‡ RÂ²:            {df_results['r2'].mean():.6f}")
        print(f"  å¹³å‡ Correlation:   {df_results['correlation'].mean():.6f}")
        print(f"  æœ€ä½³ Correlation:   {df_results['correlation'].max():.6f} ({df_results.loc[df_results['correlation'].idxmax(), 'symbol']})")
        print(f"  æœ€å·® Correlation:   {df_results['correlation'].min():.6f} ({df_results.loc[df_results['correlation'].idxmin(), 'symbol']})")
        print(f"  æ€»è®­ç»ƒæ—¶é—´:         {df_results['training_time_minutes'].sum():.2f} åˆ†é’Ÿ")
        print(f"  å¹³å‡è®­ç»ƒæ—¶é—´/æ ‡çš„:  {df_results['training_time_minutes'].mean():.2f} åˆ†é’Ÿ")
        
        # ç”Ÿæˆæ±‡æ€»æ—¶åºå›¾
        print(f"\n{'='*80}")
        print(f"ğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
        print(f"{'='*80}")
        plot_all_symbols_timeseries(output_dir, all_symbols_list, image_dir)
        
        # è¾“å‡ºæ–‡ä»¶ä½ç½®
        print(f"\n{'='*80}")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®")
        print(f"{'='*80}")
        print(f"  æ¨¡å‹æ–‡ä»¶:     {output_dir.absolute()}/")
        print(f"  CSVæ±‡æ€»:      {csv_file.absolute()}")
        print(f"  MDæ±‡æ€»:       {md_file.absolute()}")
        print(f"  æ—¥å¿—æ–‡ä»¶:     {log_dir.absolute()}/")
        print(f"  å›¾è¡¨æ–‡ä»¶:     {image_dir.absolute()}/")
        
    if failed_symbols:
        print(f"\nâŒ å¤±è´¥çš„æ ‡çš„: {failed_symbols}")
    
    print(f"\n{'='*80}")
    print(f"âœ… æ‰€æœ‰è®­ç»ƒå®Œæˆ!")
    print(f"{'='*80}")


if __name__ == '__main__':
    main()
