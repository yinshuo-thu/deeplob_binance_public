#!/usr/bin/env python3
"""
DeepLOB-TCN Hierarchical Modeling - å¤šæ—¶é—´å°ºåº¦é¢„æµ‹

ä¸»è¦æ”¹è¿›:
- Hierarchical Modeling: åŒæ—¶é¢„æµ‹return_10så’Œreturn_60s
- å±‚æ¬¡åŒ–æ¶æ„: çŸ­æœŸé¢„æµ‹ -> é•¿æœŸé¢„æµ‹
- å…±äº«ç‰¹å¾æå–: CNNç‰¹å¾æå–å±‚å…±äº«
- å¤šä»»åŠ¡å­¦ä¹ : è”åˆä¼˜åŒ–ä¸¤ä¸ªç›®æ ‡
- EMAå¹³æ»‘: å¯¹ä¸¤ä¸ªç›®æ ‡å˜é‡éƒ½è¿›è¡ŒEMAå¹³æ»‘

æ¶æ„è®¾è®¡:
1. å…±äº«CNNç‰¹å¾æå–å±‚ (DeepLOB CNNéƒ¨åˆ†)
2. çŸ­æœŸTCNåˆ†æ”¯: é¢„æµ‹return_10s (10ç§’æ”¶ç›Š)
3. é•¿æœŸTCNåˆ†æ”¯: é¢„æµ‹return_60s (60ç§’æ”¶ç›Š)
   - å¯ä»¥åŸºäºçŸ­æœŸé¢„æµ‹å’Œå…±äº«ç‰¹å¾
   - ä½¿ç”¨æ›´å¤§çš„æ„Ÿå—é‡å¤„ç†é•¿æœŸæ¨¡å¼
4. å¤šä»»åŠ¡æŸå¤±: åŠ æƒç»„åˆä¸¤ä¸ªç›®æ ‡çš„æŸå¤±

Hierarchicalæ€æƒ³:
- return_60så¯ä»¥çœ‹ä½œæ˜¯å¤šä¸ªreturn_10sçš„ç´¯ç§¯
- çŸ­æœŸé¢„æµ‹æœ‰åŠ©äºé•¿æœŸé¢„æµ‹
- å…±äº«åº•å±‚ç‰¹å¾ï¼Œåˆ†å±‚é¢„æµ‹ä¸åŒæ—¶é—´å°ºåº¦
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle
import warnings
import time
import json
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import sys
warnings.filterwarnings('ignore')

# è®¾ç½®multiprocessingå¯åŠ¨æ–¹æ³•ä¸º'spawn'ï¼ˆPyTorch CUDAè¦æ±‚ï¼‰
if __name__ == '__main__':
    mp.set_start_method('spawn', force=True)

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# å›ºå®šéšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)


# ============================================================================
# 1. Dataset with EMA Smoothing (Multi-Target)
# ============================================================================

def apply_ema_smoothing(values, alpha=0.2):
    """
    å¯¹ç›®æ ‡å˜é‡åº”ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡(EMA)å¹³æ»‘
    
    Args:
        values: åŸå§‹ç›®æ ‡å˜é‡æ•°ç»„
        alpha: å¹³æ»‘å› å­ (0 < alpha <= 1)
              - alphaè¶Šå°ï¼Œå¹³æ»‘ç¨‹åº¦è¶Šé«˜ï¼ˆæ›´å¹³æ»‘ï¼‰
              - alphaè¶Šå¤§ï¼Œå¹³æ»‘ç¨‹åº¦è¶Šä½ï¼ˆæ›´æ¥è¿‘åŸå§‹å€¼ï¼‰
              - é»˜è®¤0.2è¡¨ç¤º20%æ–°å€¼ï¼Œ80%å†å²å€¼
    
    Returns:
        smoothed: å¹³æ»‘åçš„æ•°ç»„
    """
    if len(values) == 0:
        return values
    
    smoothed = np.zeros_like(values)
    smoothed[0] = values[0]  # ç¬¬ä¸€ä¸ªå€¼ä¿æŒä¸å˜
    
    for i in range(1, len(values)):
        # EMAå…¬å¼: EMA[t] = alpha * value[t] + (1 - alpha) * EMA[t-1]
        smoothed[i] = alpha * values[i] + (1 - alpha) * smoothed[i-1]
    
    return smoothed


class HierarchicalLOBDataset(Dataset):
    """Hierarchical LOBæ•°æ®é›† - åŒæ—¶æ”¯æŒreturn_10så’Œreturn_60s"""
    
    def __init__(self, file_path, start_ratio=0.0, end_ratio=1.0, 
                 sequence_length=100, scaler=None, 
                 feature_dim=40, target_col_10s=40, target_col_60s=41,
                 fit_scaler=False, ema_alpha=0.2):
        """
        Args:
            target_col_10s: return_10sçš„åˆ—ç´¢å¼• (é»˜è®¤41)
            target_col_60s: return_60sçš„åˆ—ç´¢å¼• (é»˜è®¤42)
            ema_alpha: EMAå¹³æ»‘å› å­ï¼Œé»˜è®¤0.2
        """
        self.sequence_length = sequence_length
        self.feature_dim = feature_dim
        self.target_col_10s = target_col_10s
        self.target_col_60s = target_col_60s
        self.ema_alpha = ema_alpha
        
        data = np.load(file_path, mmap_mode='r')
        n = len(data)
        start_idx = int(n * start_ratio)
        end_idx = int(n * end_ratio)
        segment = data[start_idx:end_idx]
        
        # âœ… å¯¹ä¸¤ä¸ªç›®æ ‡å˜é‡éƒ½è¿›è¡ŒEMAå¹³æ»‘
        # å¤„ç†return_10s
        raw_targets_10s = segment[:, target_col_10s].copy()
        valid_mask_10s = np.isfinite(raw_targets_10s)
        if valid_mask_10s.sum() > 0:
            smoothed_targets_10s = np.zeros_like(raw_targets_10s)
            valid_indices_10s = np.where(valid_mask_10s)[0]
            valid_values_10s = raw_targets_10s[valid_indices_10s]
            smoothed_valid_10s = apply_ema_smoothing(valid_values_10s, alpha=ema_alpha)
            smoothed_targets_10s[valid_indices_10s] = smoothed_valid_10s
            smoothed_targets_10s[~valid_mask_10s] = raw_targets_10s[~valid_mask_10s]
        else:
            smoothed_targets_10s = raw_targets_10s
        
        # å¤„ç†return_60s
        raw_targets_60s = segment[:, target_col_60s].copy()
        valid_mask_60s = np.isfinite(raw_targets_60s)
        if valid_mask_60s.sum() > 0:
            smoothed_targets_60s = np.zeros_like(raw_targets_60s)
            valid_indices_60s = np.where(valid_mask_60s)[0]
            valid_values_60s = raw_targets_60s[valid_indices_60s]
            smoothed_valid_60s = apply_ema_smoothing(valid_values_60s, alpha=ema_alpha)
            smoothed_targets_60s[valid_indices_60s] = smoothed_valid_60s
            smoothed_targets_60s[~valid_mask_60s] = raw_targets_60s[~valid_mask_60s]
        else:
            smoothed_targets_60s = raw_targets_60s
        
        # å°†å¹³æ»‘åçš„ç›®æ ‡å˜é‡æ›¿æ¢åŸå§‹å€¼
        segment_smoothed = segment.copy()
        segment_smoothed[:, target_col_10s] = smoothed_targets_10s
        segment_smoothed[:, target_col_60s] = smoothed_targets_60s
        
        if fit_scaler:
            features = segment_smoothed[:, :feature_dim]
            targets_10s = segment_smoothed[:, target_col_10s]
            targets_60s = segment_smoothed[:, target_col_60s]
            
            valid_mask = (np.isfinite(features).all(axis=1) & 
                         np.isfinite(targets_10s) & 
                         np.isfinite(targets_60s))
            features_clean = features[valid_mask]
            
            if scaler is None:
                self.scaler = StandardScaler()
                self.scaler.fit(features_clean)
            else:
                self.scaler = scaler
        else:
            self.scaler = scaler
        
        self.data = segment_smoothed
        self.n_samples = len(self.data) - self.sequence_length
    
    def __len__(self):
        return self.n_samples
    
    def __getitem__(self, idx):
        window = self.data[idx:idx + self.sequence_length, :self.feature_dim].copy()
        target_10s = self.data[idx + self.sequence_length - 1, self.target_col_10s].copy()
        target_60s = self.data[idx + self.sequence_length - 1, self.target_col_60s].copy()
        
        if not np.isfinite(window).all() or not np.isfinite(target_10s) or not np.isfinite(target_60s):
            return (torch.zeros(1, self.sequence_length, self.feature_dim), 
                   torch.zeros(1), torch.zeros(1))
        
        if self.scaler is not None:
            window = self.scaler.transform(window)
        
        # è½¬æ¢ä¸ºBPS (Basis Points)
        target_10s = np.log1p(target_10s) * 10000
        target_60s = np.log1p(target_60s) * 10000
        
        if not np.isfinite(target_10s) or not np.isfinite(target_60s):
            return (torch.zeros(1, self.sequence_length, self.feature_dim), 
                   torch.zeros(1), torch.zeros(1))
        
        x = torch.FloatTensor(window).unsqueeze(0)
        y_10s = torch.FloatTensor([target_10s])
        y_60s = torch.FloatTensor([target_60s])
        
        return x, y_10s, y_60s


# ============================================================================
# 2. TCN Model (ä¸åŸå§‹ç‰ˆæœ¬ç›¸åŒ)
# ============================================================================

class TCNBlock(nn.Module):
    """
    TCN æ®‹å·®å— (TCN Residual Block)
    
    æ ¸å¿ƒç»„ä»¶:
    - å› æœå·ç§¯ (Causal Convolution): åªä½¿ç”¨è¿‡å»çš„ä¿¡æ¯
    - è†¨èƒ€å·ç§¯ (Dilated Convolution): æ‰©å¤§æ„Ÿå—é‡
    - æ®‹å·®è¿æ¥ (Residual Connection): ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
    """
    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout=0.2):
        super(TCNBlock, self).__init__()
        
        # å› æœå¡«å……: ç¡®ä¿åªä½¿ç”¨è¿‡å»çš„ä¿¡æ¯ï¼Œé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²
        self.padding = (kernel_size - 1) * dilation
        
        # ç¬¬ä¸€ä¸ªè†¨èƒ€å·ç§¯å±‚
        self.conv1 = nn.Conv1d(
            in_channels, out_channels, kernel_size,
            padding=0,  # æˆ‘ä»¬æ‰‹åŠ¨è¿›è¡Œå› æœå¡«å……
            dilation=dilation
        )
        self.bn1 = nn.BatchNorm1d(out_channels)
        
        # ç¬¬äºŒä¸ªè†¨èƒ€å·ç§¯å±‚
        self.conv2 = nn.Conv1d(
            out_channels, out_channels, kernel_size,
            padding=0,  # æˆ‘ä»¬æ‰‹åŠ¨è¿›è¡Œå› æœå¡«å……
            dilation=dilation
        )
        self.bn2 = nn.BatchNorm1d(out_channels)
        
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.LeakyReLU(negative_slope=0.01)
        
        # æ®‹å·®è¿æ¥: å¦‚æœè¾“å…¥è¾“å‡ºç»´åº¦ä¸åŒï¼Œéœ€è¦1x1å·ç§¯è°ƒæ•´
        self.residual = None
        if in_channels != out_channels:
            self.residual = nn.Conv1d(in_channels, out_channels, 1)
        
        # æƒé‡åˆå§‹åŒ–
        nn.init.kaiming_normal_(self.conv1.weight)
        nn.init.kaiming_normal_(self.conv2.weight)
        if self.residual is not None:
            nn.init.kaiming_normal_(self.residual.weight)
    
    def forward(self, x):
        """
        Args:
            x: (batch, channels, seq_len)
        Returns:
            out: (batch, channels, seq_len)
        """
        residual = x
        
        # ç¬¬ä¸€ä¸ªå·ç§¯ + å› æœå¡«å……
        out = F.pad(x, (self.padding, 0))
        out = self.conv1(out)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.dropout(out)
        
        # ç¬¬äºŒä¸ªå·ç§¯ + å› æœå¡«å……
        out = F.pad(out, (self.padding, 0))
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.dropout(out)
        
        # æ®‹å·®è¿æ¥
        if self.residual is not None:
            residual = self.residual(residual)
        
        out += residual
        out = self.relu(out)
        
        return out


class TCN(nn.Module):
    """
    æ—¶åºå·ç§¯ç½‘ç»œ (Temporal Convolutional Network)
    """
    def __init__(self, input_size, num_channels, kernel_size=3, dropout=0.2):
        """
        Args:
            input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
            num_channels: æ¯å±‚çš„é€šé“æ•°åˆ—è¡¨ï¼Œå¦‚ [64, 64, 64]
            kernel_size: å·ç§¯æ ¸å¤§å°
            dropout: Dropout æ¯”ç‡
        """
        super(TCN, self).__init__()
        
        layers = []
        num_levels = len(num_channels)
        
        for i in range(num_levels):
            dilation = 2 ** i  # æŒ‡æ•°å¢é•¿çš„è†¨èƒ€ç‡: 1, 2, 4, 8, ...
            in_channels = input_size if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            
            layers.append(
                TCNBlock(in_channels, out_channels, kernel_size, 
                        dilation, dropout)
            )
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        """
        Args:
            x: (batch, channels, seq_len)
        Returns:
            out: (batch, channels, seq_len)
        """
        return self.network(x)


# ============================================================================
# 3. Hierarchical Model Architecture (Optimized)
# ============================================================================

class CrossAttention(nn.Module):
    """
    äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼šè®©é•¿æœŸTCNå…³æ³¨çŸ­æœŸç‰¹å¾ä¸­çš„é‡è¦éƒ¨åˆ†
    """
    def __init__(self, query_dim, key_dim, value_dim, hidden_dim=64):
        super(CrossAttention, self).__init__()
        self.query_proj = nn.Linear(query_dim, hidden_dim)
        self.key_proj = nn.Linear(key_dim, hidden_dim)
        self.value_proj = nn.Linear(value_dim, hidden_dim)
        self.scale = hidden_dim ** -0.5
        self.out_proj = nn.Linear(hidden_dim, value_dim)
        
    def forward(self, query, key, value):
        """
        Args:
            query: (batch, seq_len, query_dim) - é•¿æœŸç‰¹å¾
            key: (batch, seq_len, key_dim) - çŸ­æœŸç‰¹å¾
            value: (batch, seq_len, value_dim) - çŸ­æœŸç‰¹å¾
        Returns:
            attended: (batch, seq_len, value_dim) - æ³¨æ„åŠ›åŠ æƒçš„çŸ­æœŸç‰¹å¾
        """
        Q = self.query_proj(query)  # (batch, seq_len, hidden_dim)
        K = self.key_proj(key)  # (batch, seq_len, hidden_dim)
        V = self.value_proj(value)  # (batch, seq_len, hidden_dim)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.bmm(Q, K.transpose(1, 2)) * self.scale  # (batch, seq_len, seq_len)
        attn_weights = F.softmax(scores, dim=-1)  # (batch, seq_len, seq_len)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attended = torch.bmm(attn_weights, V)  # (batch, seq_len, hidden_dim)
        attended = self.out_proj(attended)  # (batch, seq_len, value_dim)
        
        return attended


class GatedFusion(nn.Module):
    """
    é—¨æ§èåˆæœºåˆ¶ï¼šæ§åˆ¶çŸ­æœŸç‰¹å¾å¦‚ä½•èå…¥é•¿æœŸé¢„æµ‹
    """
    def __init__(self, shared_dim, short_dim, output_dim):
        super(GatedFusion, self).__init__()
        self.gate = nn.Sequential(
            nn.Linear(shared_dim + short_dim, output_dim),
            nn.Sigmoid()
        )
        self.transform = nn.Linear(shared_dim + short_dim, output_dim)
        
    def forward(self, shared_feat, short_feat):
        """
        Args:
            shared_feat: (batch, seq_len, shared_dim)
            short_feat: (batch, seq_len, short_dim)
        Returns:
            fused: (batch, seq_len, output_dim)
        """
        combined = torch.cat([shared_feat, short_feat], dim=-1)  # (batch, seq_len, shared_dim + short_dim)
        gate = self.gate(combined)  # (batch, seq_len, output_dim)
        transformed = self.transform(combined)  # (batch, seq_len, output_dim)
        fused = gate * transformed  # é—¨æ§èåˆ
        return fused


class DeepLOB_Hierarchical_TCN(nn.Module):
    """
    DeepLOB-TCN Hierarchical Model (Optimized)
    
    æ¶æ„æµç¨‹:
    1. å…±äº«CNNç‰¹å¾æå– (DeepLOB CNNéƒ¨åˆ†)
    2. çŸ­æœŸTCNåˆ†æ”¯: é¢„æµ‹return_10s
    3. é•¿æœŸTCNåˆ†æ”¯: é¢„æµ‹return_60s (åŸºäºçŸ­æœŸé¢„æµ‹å’Œå…±äº«ç‰¹å¾)
    4. ä¼˜åŒ–ç»„ä»¶:
       - äº¤å‰æ³¨æ„åŠ›æœºåˆ¶: é•¿æœŸTCNå…³æ³¨çŸ­æœŸç‰¹å¾ä¸­çš„é‡è¦éƒ¨åˆ†
       - é—¨æ§èåˆæœºåˆ¶: æ§åˆ¶çŸ­æœŸç‰¹å¾å¦‚ä½•èå…¥é•¿æœŸé¢„æµ‹
       - æ®‹å·®è¿æ¥: çŸ­æœŸé¢„æµ‹ä¿¡æ¯ç›´æ¥ä¼ é€’ç»™é•¿æœŸé¢„æµ‹
    """
    def __init__(self, input_channels=1, dropout=0.3, 
                 short_term_channels=[64, 64, 64, 64],
                 long_term_channels=[64, 64, 64, 64, 64],
                 use_attention=True, use_gated_fusion=True, use_residual=True):
        """
        Args:
            short_term_channels: çŸ­æœŸTCNçš„é€šé“æ•°åˆ—è¡¨
            long_term_channels: é•¿æœŸTCNçš„é€šé“æ•°åˆ—è¡¨ï¼ˆå¯ä»¥æ›´æ·±ï¼‰
            use_attention: æ˜¯å¦ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
            use_gated_fusion: æ˜¯å¦ä½¿ç”¨é—¨æ§èåˆæœºåˆ¶
            use_residual: æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥
        """
        super(DeepLOB_Hierarchical_TCN, self).__init__()
        
        self.use_attention = use_attention
        self.use_gated_fusion = use_gated_fusion
        self.use_residual = use_residual
        
        # ==================== å…±äº«CNNç‰¹å¾æå–éƒ¨åˆ† ====================
        # First convolutional block
        self.conv1a = nn.Conv2d(input_channels, 32, kernel_size=(1, 2), stride=(1, 2))
        self.bn1a = nn.BatchNorm2d(32)
        self.conv1b = nn.Conv2d(32, 32, kernel_size=(4, 1), padding=(0, 0))
        self.bn1b = nn.BatchNorm2d(32)
        self.conv1c = nn.Conv2d(32, 32, kernel_size=(4, 1), padding=(0, 0))
        self.bn1c = nn.BatchNorm2d(32)
        
        # Second convolutional block
        self.conv2a = nn.Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 2))
        self.bn2a = nn.BatchNorm2d(32)
        self.conv2b = nn.Conv2d(32, 32, kernel_size=(4, 1), padding=(0, 0))
        self.bn2b = nn.BatchNorm2d(32)
        self.conv2c = nn.Conv2d(32, 32, kernel_size=(4, 1), padding=(0, 0))
        self.bn2c = nn.BatchNorm2d(32)
        
        # Third convolutional block
        self.conv3a = nn.Conv2d(32, 32, kernel_size=(1, 10))
        self.bn3a = nn.BatchNorm2d(32)
        
        # Inception module
        self.inception1 = nn.Conv2d(32, 64, kernel_size=(1, 1), stride=1)
        self.bn_inc1 = nn.BatchNorm2d(64)
        
        self.inception2a = nn.Conv2d(32, 64, kernel_size=(1, 1), stride=1)
        self.inception2b = nn.Conv2d(64, 64, kernel_size=(3, 1), stride=1, padding=(1, 0))
        self.bn_inc2 = nn.BatchNorm2d(64)
        
        self.inception3a = nn.Conv2d(32, 64, kernel_size=(1, 1), stride=1)
        self.inception3b = nn.Conv2d(64, 64, kernel_size=(5, 1), stride=1, padding=(2, 0))
        self.bn_inc3 = nn.BatchNorm2d(64)
        
        self.inception4 = nn.MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
        self.inception4_conv = nn.Conv2d(32, 64, kernel_size=(1, 1), stride=1)
        self.bn_inc4 = nn.BatchNorm2d(64)
        
        # ==================== çŸ­æœŸTCNåˆ†æ”¯ (return_10s) ====================
        self.tcn_short = TCN(
            input_size=256,
            num_channels=short_term_channels,
            kernel_size=3,
            dropout=dropout
        )
        
        # çŸ­æœŸé¢„æµ‹å¤´
        self.fc_short_1 = nn.Linear(short_term_channels[-1], 64)
        self.bn_short_1 = nn.BatchNorm1d(64)
        self.fc_short_2 = nn.Linear(64, 1)
        self.dropout_short = nn.Dropout(dropout)
        
        # ==================== é•¿æœŸTCNåˆ†æ”¯ (return_60s) ====================
        # äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¯é€‰ï¼‰
        if use_attention:
            self.cross_attention = CrossAttention(
                query_dim=256,  # å…±äº«ç‰¹å¾ç»´åº¦
                key_dim=short_term_channels[-1],  # çŸ­æœŸç‰¹å¾ç»´åº¦
                value_dim=short_term_channels[-1],
                hidden_dim=64
            )
        
        # é—¨æ§èåˆæœºåˆ¶ï¼ˆå¯é€‰ï¼‰
        if use_gated_fusion:
            self.gated_fusion = GatedFusion(
                shared_dim=256,
                short_dim=short_term_channels[-1],
                output_dim=256
            )
        
        # é•¿æœŸTCNè¾“å…¥ç»´åº¦
        if use_attention or use_gated_fusion:
            # å¦‚æœä½¿ç”¨æ³¨æ„åŠ›æˆ–é—¨æ§èåˆï¼Œè¾“å…¥ç»´åº¦ä¿æŒä¸º256
            long_tcn_input_size = 256
        else:
            # å¦åˆ™ï¼Œç®€å•æ‹¼æ¥
            long_tcn_input_size = 256 + short_term_channels[-1]
        
        self.tcn_long = TCN(
            input_size=long_tcn_input_size,
            num_channels=long_term_channels,
            kernel_size=3,
            dropout=dropout
        )
        
        # é•¿æœŸé¢„æµ‹å¤´
        self.fc_long_1 = nn.Linear(long_term_channels[-1], 64)
        self.bn_long_1 = nn.BatchNorm1d(64)
        self.fc_long_2 = nn.Linear(64, 1)
        self.dropout_long = nn.Dropout(dropout)
        
        # æ®‹å·®è¿æ¥ï¼šçŸ­æœŸé¢„æµ‹åˆ°é•¿æœŸé¢„æµ‹ï¼ˆå¯é€‰ï¼‰
        if use_residual:
            self.residual_proj = nn.Linear(1, 1)  # å°†çŸ­æœŸé¢„æµ‹æŠ•å½±åˆ°é•¿æœŸé¢„æµ‹ç©ºé—´
    
    def forward(self, x):
        """
        Args:
            x: (batch, 1, seq_len, feature_dim)
        Returns:
            pred_10s: (batch, 1) - return_10sé¢„æµ‹
            pred_60s: (batch, 1) - return_60sé¢„æµ‹
        """
        # ==================== å…±äº«CNNç‰¹å¾æå– ====================
        x = F.leaky_relu(self.bn1a(self.conv1a(x)), negative_slope=0.01)
        x = F.leaky_relu(self.bn1b(self.conv1b(x)), negative_slope=0.01)
        x = F.leaky_relu(self.bn1c(self.conv1c(x)), negative_slope=0.01)
        
        x = F.leaky_relu(self.bn2a(self.conv2a(x)), negative_slope=0.01)
        x = F.leaky_relu(self.bn2b(self.conv2b(x)), negative_slope=0.01)
        x = F.leaky_relu(self.bn2c(self.conv2c(x)), negative_slope=0.01)
        
        x = F.leaky_relu(self.bn3a(self.conv3a(x)), negative_slope=0.01)
        
        # Inception module
        branch1 = F.leaky_relu(self.bn_inc1(self.inception1(x)), negative_slope=0.01)
        branch2 = F.leaky_relu(self.inception2a(x), negative_slope=0.01)
        branch2 = F.leaky_relu(self.bn_inc2(self.inception2b(branch2)), negative_slope=0.01)
        branch3 = F.leaky_relu(self.inception3a(x), negative_slope=0.01)
        branch3 = F.leaky_relu(self.bn_inc3(self.inception3b(branch3)), negative_slope=0.01)
        branch4 = self.inception4(x)
        branch4 = F.leaky_relu(self.bn_inc4(self.inception4_conv(branch4)), negative_slope=0.01)
        
        shared_features = torch.cat([branch1, branch2, branch3, branch4], dim=1)
        # Reshape for TCN: (batch, 256, seq_len)
        shared_features = shared_features.squeeze(-1)  # (batch, 256, seq_len)
        
        # ==================== çŸ­æœŸTCNåˆ†æ”¯ (return_10s) ====================
        short_tcn_out = self.tcn_short(shared_features)  # (batch, 64, seq_len)
        short_features = short_tcn_out[:, :, -1]  # (batch, 64) - å–æœ€åæ—¶é—´æ­¥
        
        # çŸ­æœŸé¢„æµ‹
        short_pred = F.leaky_relu(self.bn_short_1(self.fc_short_1(short_features)), negative_slope=0.01)
        short_pred = self.dropout_short(short_pred)
        pred_10s = self.fc_short_2(short_pred)
        
        # ==================== é•¿æœŸTCNåˆ†æ”¯ (return_60s) ====================
        # å‡†å¤‡é•¿æœŸTCNè¾“å…¥
        seq_len = shared_features.size(2)
        
        # å°†çŸ­æœŸç‰¹å¾æ‰©å±•åˆ°æ¯ä¸ªæ—¶é—´æ­¥: (batch, 64) -> (batch, seq_len, 64)
        short_features_seq = short_tcn_out.permute(0, 2, 1)  # (batch, seq_len, 64)
        shared_features_seq = shared_features.permute(0, 2, 1)  # (batch, seq_len, 256)
        
        # ä¼˜åŒ–1: äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
        if self.use_attention:
            # é•¿æœŸTCNå…³æ³¨çŸ­æœŸç‰¹å¾ä¸­çš„é‡è¦éƒ¨åˆ†
            attended_short = self.cross_attention(
                query=shared_features_seq,  # é•¿æœŸç‰¹å¾ä½œä¸ºquery
                key=short_features_seq,      # çŸ­æœŸç‰¹å¾ä½œä¸ºkey
                value=short_features_seq    # çŸ­æœŸç‰¹å¾ä½œä¸ºvalue
            )  # (batch, seq_len, 64)
            
            # èåˆå…±äº«ç‰¹å¾å’Œæ³¨æ„åŠ›åŠ æƒçš„çŸ­æœŸç‰¹å¾
            if self.use_gated_fusion:
                # ä¼˜åŒ–2: é—¨æ§èåˆæœºåˆ¶
                long_tcn_input = self.gated_fusion(
                    shared_feat=shared_features_seq,
                    short_feat=attended_short
                )  # (batch, seq_len, 256)
            else:
                # ç®€å•æ‹¼æ¥
                long_tcn_input = torch.cat([shared_features_seq, attended_short], dim=-1)  # (batch, seq_len, 320)
        elif self.use_gated_fusion:
            # åªä½¿ç”¨é—¨æ§èåˆï¼Œä¸ä½¿ç”¨æ³¨æ„åŠ›
            long_tcn_input = self.gated_fusion(
                shared_feat=shared_features_seq,
                short_feat=short_features_seq
            )  # (batch, seq_len, 256)
        else:
            # ç®€å•æ‹¼æ¥ï¼ˆåŸå§‹æ–¹æ³•ï¼‰
            short_features_expanded = short_features.unsqueeze(1).expand(-1, seq_len, -1)  # (batch, seq_len, 64)
            long_tcn_input = torch.cat([shared_features_seq, short_features_expanded], dim=-1)  # (batch, seq_len, 320)
        
        # è½¬æ¢å›TCNè¾“å…¥æ ¼å¼: (batch, channels, seq_len)
        long_tcn_input = long_tcn_input.permute(0, 2, 1)  # (batch, channels, seq_len)
        
        long_tcn_out = self.tcn_long(long_tcn_input)  # (batch, 64, seq_len)
        long_features = long_tcn_out[:, :, -1]  # (batch, 64) - å–æœ€åæ—¶é—´æ­¥
        
        # é•¿æœŸé¢„æµ‹
        long_pred = F.leaky_relu(self.bn_long_1(self.fc_long_1(long_features)), negative_slope=0.01)
        long_pred = self.dropout_long(long_pred)
        pred_60s_base = self.fc_long_2(long_pred)
        
        # ä¼˜åŒ–3: æ®‹å·®è¿æ¥ï¼ˆçŸ­æœŸé¢„æµ‹åˆ°é•¿æœŸé¢„æµ‹ï¼‰
        if self.use_residual:
            # å°†çŸ­æœŸé¢„æµ‹æŠ•å½±å¹¶æ·»åŠ åˆ°é•¿æœŸé¢„æµ‹
            pred_60s_residual = self.residual_proj(pred_10s)
            pred_60s = pred_60s_base + pred_60s_residual
        else:
            pred_60s = pred_60s_base
        
        return pred_10s, pred_60s


# ============================================================================
# 4. Training Function (å•è¿›ç¨‹ç‰ˆæœ¬ï¼Œç”¨äºå¹¶è¡Œè°ƒç”¨)
# ============================================================================

def train_single_symbol_worker(args):
    """å•æ ‡çš„è®­ç»ƒå‡½æ•°ï¼ˆç”¨äºå¹¶è¡Œè°ƒç”¨ï¼‰"""
    symbol, data_dir, output_dir, log_dir, config, gpu_id = args
    
    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„GPU
    if gpu_id >= torch.cuda.device_count():
        gpu_id = 0
    
    device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')
    
    # è®¾ç½®æ—¥å¿—æ–‡ä»¶
    log_file = log_dir / f"{symbol}_training.log"
    log_f = open(log_file, 'w')
    
    def log_print(*args, **kwargs):
        msg = ' '.join(str(a) for a in args)
        print(msg)
        log_f.write(msg + '\n')
        log_f.flush()
    
    try:
        log_print(f"\n{'='*80}")
        log_print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {symbol} (GPU {gpu_id}) - DeepLOB-TCN Hierarchical Model (Optimized)")
        log_print(f"   EMA Alpha: {config.get('ema_alpha', 0.2)}")
        log_print(f"   Loss Weight (10s/60s): {config.get('loss_weight_10s', 0.5)}/{config.get('loss_weight_60s', 0.5)}")
        log_print(f"   Use Attention: {config.get('use_attention', True)}")
        log_print(f"   Use Gated Fusion: {config.get('use_gated_fusion', True)}")
        log_print(f"   Use Residual: {config.get('use_residual', True)}")
        log_print(f"   Adaptive Loss Weight: {config.get('adaptive_loss_weight', False)}")
        log_print(f"{'='*80}")
        
        start_time = time.time()
        
        # æ–‡ä»¶è·¯å¾„
        data_file = Path(data_dir) / f"{symbol}_20250801_20250810.npy"
        if not data_file.exists():
            log_print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return None
        
        # åˆ›å»ºæ•°æ®é›† - Hierarchicalå¤šç›®æ ‡
        log_print(f"   ğŸ“‚ åŠ è½½æ•°æ® (Hierarchical, EMAå¹³æ»‘: alpha={config.get('ema_alpha', 0.2)})...")
        train_dataset = HierarchicalLOBDataset(
            data_file, start_ratio=0.0, end_ratio=0.6,
            sequence_length=config['sequence_length'], 
            fit_scaler=True,
            ema_alpha=config.get('ema_alpha', 0.2)
        )
        
        val_dataset = HierarchicalLOBDataset(
            data_file, start_ratio=0.6, end_ratio=0.8,
            sequence_length=config['sequence_length'],
            scaler=train_dataset.scaler,
            fit_scaler=False,
            ema_alpha=config.get('ema_alpha', 0.2)
        )
        
        test_dataset = HierarchicalLOBDataset(
            data_file, start_ratio=0.8, end_ratio=1.0,
            sequence_length=config['sequence_length'],
            scaler=train_dataset.scaler,
            fit_scaler=False,
            ema_alpha=config.get('ema_alpha', 0.2)
        )
        
        log_print(f"      Train: {len(train_dataset):,} samples")
        log_print(f"      Val:   {len(val_dataset):,} samples")
        log_print(f"      Test:  {len(test_dataset):,} samples")
        
        # DataLoaders
        train_loader = DataLoader(
            train_dataset, batch_size=config['batch_size'],
            shuffle=True, num_workers=config['num_workers'], pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset, batch_size=config['batch_size'],
            shuffle=False, num_workers=config['num_workers'], pin_memory=True
        )
        
        test_loader = DataLoader(
            test_dataset, batch_size=config['batch_size'],
            shuffle=False, num_workers=config['num_workers'], pin_memory=True
        )
        
        # åˆ›å»ºæ¨¡å‹
        log_print(f"   ğŸ—ï¸  åˆ›å»ºæ¨¡å‹... (DeepLOB-TCN Hierarchical Optimized)")
        model = DeepLOB_Hierarchical_TCN(
            input_channels=1, 
            dropout=config['dropout'],
            short_term_channels=config.get('short_term_channels', [64, 64, 64, 64]),
            long_term_channels=config.get('long_term_channels', [64, 64, 64, 64, 64]),
            use_attention=config.get('use_attention', True),
            use_gated_fusion=config.get('use_gated_fusion', True),
            use_residual=config.get('use_residual', True)
        ).to(device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])
        criterion = nn.HuberLoss(delta=1.0)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=3
        )
        
        # æŸå¤±æƒé‡ï¼ˆæ”¯æŒè‡ªé€‚åº”è°ƒæ•´ï¼‰
        adaptive_loss_weight = config.get('adaptive_loss_weight', False)
        loss_weight_10s = config.get('loss_weight_10s', 0.5)
        loss_weight_60s = config.get('loss_weight_60s', 0.5)
        
        # è‡ªé€‚åº”æŸå¤±æƒé‡ï¼šåˆå§‹æƒé‡å’Œè°ƒæ•´å‚æ•°
        if adaptive_loss_weight:
            initial_weight_10s = loss_weight_10s
            initial_weight_60s = loss_weight_60s
            log_print(f"   ğŸ“Š è‡ªé€‚åº”æŸå¤±æƒé‡å·²å¯ç”¨")
        
        # è®­ç»ƒå¾ªç¯
        log_print(f"   ğŸ‹ï¸  å¼€å§‹è®­ç»ƒ...")
        best_val_loss = float('inf')
        patience_counter = 0
        epoch_times = []
        history = {
            'train_loss': [],
            'train_loss_10s': [],
            'train_loss_60s': [],
            'val_loss': [],
            'val_loss_10s': [],
            'val_loss_60s': [],
            'lr': [],
            'epoch_times': []
        }
        
        for epoch in range(config['num_epochs']):
            epoch_start = time.time()
            
            # è‡ªé€‚åº”æŸå¤±æƒé‡è°ƒæ•´ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if adaptive_loss_weight and epoch > 0:
                # æ ¹æ®ä¸Šä¸€è½®çš„æŸå¤±æ¯”ä¾‹åŠ¨æ€è°ƒæ•´æƒé‡
                # å¦‚æœloss_10sç›¸å¯¹è¾ƒå¤§ï¼Œå¢åŠ å…¶æƒé‡ï¼›åä¹‹äº¦ç„¶
                if history['train_loss_10s'][-1] > 0 and history['train_loss_60s'][-1] > 0:
                    ratio_10s = history['train_loss_10s'][-1] / (history['train_loss_10s'][-1] + history['train_loss_60s'][-1])
                    ratio_60s = history['train_loss_60s'][-1] / (history['train_loss_10s'][-1] + history['train_loss_60s'][-1])
                    # å¹³æ»‘è°ƒæ•´ï¼šä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡
                    alpha = 0.1  # è°ƒæ•´é€Ÿåº¦
                    loss_weight_10s = (1 - alpha) * loss_weight_10s + alpha * ratio_10s
                    loss_weight_60s = (1 - alpha) * loss_weight_60s + alpha * ratio_60s
                    # å½’ä¸€åŒ–
                    total_weight = loss_weight_10s + loss_weight_60s
                    loss_weight_10s = loss_weight_10s / total_weight
                    loss_weight_60s = loss_weight_60s / total_weight
            
            # è®­ç»ƒ
            model.train()
            train_loss = 0.0
            train_loss_10s = 0.0
            train_loss_60s = 0.0
            for batch_x, batch_y_10s, batch_y_60s in train_loader:
                batch_x = batch_x.to(device)
                batch_y_10s = batch_y_10s.to(device)
                batch_y_60s = batch_y_60s.to(device)
                
                optimizer.zero_grad()
                pred_10s, pred_60s = model(batch_x)
                
                loss_10s = criterion(pred_10s.squeeze(), batch_y_10s.squeeze())
                loss_60s = criterion(pred_60s.squeeze(), batch_y_60s.squeeze())
                loss = loss_weight_10s * loss_10s + loss_weight_60s * loss_60s
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += loss.item()
                train_loss_10s += loss_10s.item()
                train_loss_60s += loss_60s.item()
            
            train_loss /= len(train_loader)
            train_loss_10s /= len(train_loader)
            train_loss_60s /= len(train_loader)
            
            # éªŒè¯
            model.eval()
            val_loss = 0.0
            val_loss_10s = 0.0
            val_loss_60s = 0.0
            with torch.no_grad():
                for batch_x, batch_y_10s, batch_y_60s in val_loader:
                    batch_x = batch_x.to(device)
                    batch_y_10s = batch_y_10s.to(device)
                    batch_y_60s = batch_y_60s.to(device)
                    
                    pred_10s, pred_60s = model(batch_x)
                    
                    loss_10s = criterion(pred_10s.squeeze(), batch_y_10s.squeeze())
                    loss_60s = criterion(pred_60s.squeeze(), batch_y_60s.squeeze())
                    loss = loss_weight_10s * loss_10s + loss_weight_60s * loss_60s
                    
                    val_loss += loss.item()
                    val_loss_10s += loss_10s.item()
                    val_loss_60s += loss_60s.item()
            
            val_loss /= len(val_loader)
            val_loss_10s /= len(val_loader)
            val_loss_60s /= len(val_loader)
            
            scheduler.step(val_loss)
            current_lr = optimizer.param_groups[0]['lr']
            
            epoch_time = time.time() - epoch_start
            epoch_times.append(epoch_time)
            
            history['train_loss'].append(train_loss)
            history['train_loss_10s'].append(train_loss_10s)
            history['train_loss_60s'].append(train_loss_60s)
            history['val_loss'].append(val_loss)
            history['val_loss_10s'].append(val_loss_10s)
            history['val_loss_60s'].append(val_loss_60s)
            history['lr'].append(current_lr)
            history['epoch_times'].append(epoch_time)
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'scaler': train_dataset.scaler,
                    'config': config
                }, output_dir / f"{symbol}_best_model.pth")
            else:
                patience_counter += 1
            
            if (epoch + 1) % 5 == 0 or epoch == 0:
                weight_info = ""
                if adaptive_loss_weight:
                    weight_info = f" | Weights: {loss_weight_10s:.3f}/{loss_weight_60s:.3f}"
                log_print(f"      Epoch {epoch+1:2d}/{config['num_epochs']} | "
                          f"Train: {train_loss:.6f} (10s: {train_loss_10s:.6f}, 60s: {train_loss_60s:.6f}) | "
                          f"Val: {val_loss:.6f} (10s: {val_loss_10s:.6f}, 60s: {val_loss_60s:.6f}) | "
                          f"LR: {current_lr:.6f}{weight_info} | Time: {epoch_time:.2f}s")
            
            if patience_counter >= config['early_stopping_patience']:
                log_print(f"      â¹ï¸  Early stopping at epoch {epoch+1}")
                break
        
        if epoch_times:
            avg_epoch_time = np.mean(epoch_times)
            log_print(f"      â±ï¸  å¹³å‡æ¯ä¸ªEpoch: {avg_epoch_time:.2f}ç§’")
        
        # æµ‹è¯•
        log_print(f"   ğŸ“Š æµ‹è¯•æœ€ä½³æ¨¡å‹...")
        checkpoint = torch.load(output_dir / f"{symbol}_best_model.pth", weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        test_preds_10s = []
        test_targets_10s = []
        test_preds_60s = []
        test_targets_60s = []
        test_loss = 0.0
        test_loss_10s = 0.0
        test_loss_60s = 0.0
        
        with torch.no_grad():
            for batch_x, batch_y_10s, batch_y_60s in test_loader:
                batch_x = batch_x.to(device)
                batch_y_10s = batch_y_10s.to(device)
                batch_y_60s = batch_y_60s.to(device)
                
                pred_10s, pred_60s = model(batch_x)
                
                loss_10s = criterion(pred_10s.squeeze(), batch_y_10s.squeeze())
                loss_60s = criterion(pred_60s.squeeze(), batch_y_60s.squeeze())
                loss = loss_weight_10s * loss_10s + loss_weight_60s * loss_60s
                
                test_loss += loss.item()
                test_loss_10s += loss_10s.item()
                test_loss_60s += loss_60s.item()
                
                test_preds_10s.append(pred_10s.cpu().numpy())
                test_targets_10s.append(batch_y_10s.cpu().numpy())
                test_preds_60s.append(pred_60s.cpu().numpy())
                test_targets_60s.append(batch_y_60s.cpu().numpy())
        
        test_loss /= len(test_loader)
        test_loss_10s /= len(test_loader)
        test_loss_60s /= len(test_loader)
        
        test_preds_10s = np.concatenate(test_preds_10s).flatten()
        test_targets_10s = np.concatenate(test_targets_10s).flatten()
        test_preds_60s = np.concatenate(test_preds_60s).flatten()
        test_targets_60s = np.concatenate(test_targets_60s).flatten()
        
        # è®¡ç®—æŒ‡æ ‡ - return_10s
        mae_10s = mean_absolute_error(test_targets_10s, test_preds_10s)
        rmse_10s = np.sqrt(mean_squared_error(test_targets_10s, test_preds_10s))
        r2_10s = r2_score(test_targets_10s, test_preds_10s)
        corr_10s = np.corrcoef(test_targets_10s, test_preds_10s)[0, 1] if len(test_targets_10s) > 1 else 0.0
        
        # è®¡ç®—æŒ‡æ ‡ - return_60s
        mae_60s = mean_absolute_error(test_targets_60s, test_preds_60s)
        rmse_60s = np.sqrt(mean_squared_error(test_targets_60s, test_preds_60s))
        r2_60s = r2_score(test_targets_60s, test_preds_60s)
        corr_60s = np.corrcoef(test_targets_60s, test_preds_60s)[0, 1] if len(test_targets_60s) > 1 else 0.0
        
        training_time = time.time() - start_time
        
        log_print(f"\n   âœ… è®­ç»ƒå®Œæˆ!")
        log_print(f"      Test Loss: {test_loss:.6f} (10s: {test_loss_10s:.6f}, 60s: {test_loss_60s:.6f})")
        log_print(f"      Return 10s - MAE: {mae_10s:.6f}, RMSE: {rmse_10s:.6f}, RÂ²: {r2_10s:.6f}, Corr: {corr_10s:.6f}")
        log_print(f"      Return 60s - MAE: {mae_60s:.6f}, RMSE: {rmse_60s:.6f}, RÂ²: {r2_60s:.6f}, Corr: {corr_60s:.6f}")
        log_print(f"      Time: {training_time/60:.2f} min")
        
        # ä¿å­˜ç»“æœ
        result = {
            'symbol': symbol,
            'test_loss': float(test_loss),
            'test_loss_10s': float(test_loss_10s),
            'test_loss_60s': float(test_loss_60s),
            'mae_10s': float(mae_10s),
            'rmse_10s': float(rmse_10s),
            'r2_10s': float(r2_10s),
            'correlation_10s': float(corr_10s),
            'mae_60s': float(mae_60s),
            'rmse_60s': float(rmse_60s),
            'r2_60s': float(r2_60s),
            'correlation_60s': float(corr_60s),
            'best_val_loss': float(best_val_loss),
            'training_time_minutes': float(training_time / 60),
            'epochs_trained': len(history['train_loss']),
            'train_samples': len(train_dataset),
            'val_samples': len(val_dataset),
            'test_samples': len(test_dataset),
            'ema_alpha': config.get('ema_alpha', 0.2)
        }
        
        # ä¿å­˜å†å²
        with open(output_dir / f"{symbol}_history.pkl", 'wb') as f:
            pickle.dump(history, f)
        
        # ä¿å­˜é¢„æµ‹
        np.savez(
            output_dir / f"{symbol}_predictions.npz",
            predictions_10s=test_preds_10s,
            targets_10s=test_targets_10s,
            predictions_60s=test_preds_60s,
            targets_60s=test_targets_60s
        )
        
        log_f.close()
        return result
        
    except Exception as e:
        log_print(f"   âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        log_print(traceback.format_exc())
        log_f.close()
        return None


# ============================================================================
# 5. Report Generation Functions
# ============================================================================

def generate_single_timescale_report(df, timescale, output_dir, image_dir):
    """ä¸ºå•ä¸ªæ—¶é—´å°ºåº¦ï¼ˆ10sæˆ–60sï¼‰ç”Ÿæˆç±»ä¼¼7ç³»åˆ—çš„å›¾è¡¨å’Œè¡¨æ ¼"""
    corr_col = f'correlation_{timescale}'
    mae_col = f'mae_{timescale}'
    rmse_col = f'rmse_{timescale}'
    r2_col = f'r2_{timescale}'
    
    # å‡†å¤‡æ•°æ®ï¼ˆç±»ä¼¼7ç³»åˆ—çš„æ ¼å¼ï¼‰
    df_single = pd.DataFrame({
        'Symbol': df['symbol'],
        'Correlation': df[corr_col],
        'MAE': df[mae_col],
        'RMSE': df[rmse_col],
        'RÂ²': df[r2_col]
    })
    df_single = df_single.sort_values('Correlation', ascending=False).reset_index(drop=True)
    
    # ============================================================================
    # Figure: Core Performance (2x2 layout) - ç±»ä¼¼7ç³»åˆ—
    # ============================================================================
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1a) Correlation by Symbol
    ax = axes[0, 0]
    colors = ['#27ae60' if x > 0.15 else '#3498db' if x > 0.05 else '#e74c3c' for x in df_single['Correlation']]
    bars = ax.barh(range(len(df_single)), df_single['Correlation'], color=colors, alpha=0.85, edgecolor='black', linewidth=0.8)
    ax.set_yticks(range(len(df_single)))
    ax.set_yticklabels(df_single['Symbol'], fontsize=9)
    ax.set_xlabel('Correlation Coefficient', fontweight='bold', fontsize=12)
    ax.set_title('(A) Prediction Correlation by Symbol', fontsize=14, fontweight='bold', pad=12)
    ax.axvline(0, color='black', linestyle='-', linewidth=1.2)
    ax.axvline(df_single['Correlation'].mean(), color='red', linestyle='--', linewidth=2.5, 
                label=f'Mean: {df_single["Correlation"].mean():.3f}', alpha=0.8)
    ax.grid(True, alpha=0.25, axis='x', linestyle='-', linewidth=0.8)
    ax.legend(loc='lower right', fontsize=11, framealpha=0.9)
    if len(df_single) > 0:
        ax.set_xlim(-0.05, max(0.30, df_single['Correlation'].max() * 1.2))
    
    # 1b) MAE vs Correlation (Scatter with size by RMSE)
    ax = axes[0, 1]
    scatter = ax.scatter(df_single['Correlation'], df_single['MAE'], s=df_single['RMSE']*30, 
                        c=df_single['Correlation'], cmap='RdYlGn', alpha=0.7, 
                        edgecolor='black', linewidth=1.2)
    # Annotate top performers
    for idx, row in df_single.head(min(3, len(df_single))).iterrows():
        ax.annotate(row['Symbol'], (row['Correlation'], row['MAE']), 
                    fontsize=9, fontweight='bold', ha='right', va='bottom',
                    xytext=(-5, 5), textcoords='offset points')
    ax.set_xlabel('Correlation Coefficient', fontweight='bold', fontsize=12)
    ax.set_ylabel('Mean Absolute Error (BPS)', fontweight='bold', fontsize=12)
    ax.set_title('(B) Prediction Accuracy vs Correlation', fontsize=14, fontweight='bold', pad=12)
    ax.grid(True, alpha=0.25, linestyle='-', linewidth=0.8)
    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label('Correlation', fontweight='bold', fontsize=11)
    
    # 1c) MAE by Symbol (Sorted)
    ax = axes[1, 0]
    df_sorted_mae = df_single.sort_values('MAE')
    colors_mae = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(df_sorted_mae)))
    ax.barh(range(len(df_sorted_mae)), df_sorted_mae['MAE'], 
           color=colors_mae, edgecolor='black', linewidth=0.6, alpha=0.85)
    ax.set_yticks(range(len(df_sorted_mae)))
    ax.set_yticklabels(df_sorted_mae['Symbol'], fontsize=9)
    ax.set_xlabel('Mean Absolute Error (BPS)', fontweight='bold', fontsize=12)
    ax.set_title('(C) MAE by Symbol (Sorted)', fontsize=14, fontweight='bold', pad=12)
    ax.axvline(df_single['MAE'].mean(), color='red', linestyle='--', linewidth=2.5, 
              label=f'Mean: {df_single["MAE"].mean():.2f}', alpha=0.8)
    ax.legend(fontsize=11, framealpha=0.9)
    ax.grid(True, alpha=0.25, axis='x', linestyle='-', linewidth=0.8)
    
    # 1d) Correlation vs MAE Relationship
    ax = axes[1, 1]
    ax.scatter(df_single['Correlation'], df_single['MAE'], s=150, alpha=0.7, 
              c=df_single['Correlation'], cmap='RdYlGn', edgecolor='black', linewidth=1.5)
    
    # Add regression line
    if len(df_single) > 1:
        z = np.polyfit(df_single['Correlation'], df_single['MAE'], 1)
        p = np.poly1d(z)
        x_line = np.linspace(df_single['Correlation'].min(), df_single['Correlation'].max(), 100)
        ax.plot(x_line, p(x_line), 'r--', linewidth=3, alpha=0.8, 
               label=f'Trend: y={z[0]:.2f}x+{z[1]:.2f}')
    
    # Annotate top performers
    for idx, row in df_single.head(min(3, len(df_single))).iterrows():
        ax.annotate(row['Symbol'], (row['Correlation'], row['MAE']), 
                   fontsize=10, fontweight='bold', ha='right', va='bottom',
                   xytext=(-8, 8), textcoords='offset points',
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
    
    ax.set_xlabel('Correlation Coefficient', fontweight='bold', fontsize=12)
    ax.set_ylabel('Mean Absolute Error (BPS)', fontweight='bold', fontsize=12)
    ax.set_title('(D) Correlation vs MAE Relationship', fontsize=14, fontweight='bold', pad=12)
    ax.legend(fontsize=11, framealpha=0.9, loc='upper right')
    ax.grid(True, alpha=0.25, linestyle='-', linewidth=0.8)
    
    plt.suptitle(f'DeepLOB-TCN Hierarchical Model - Return {timescale} Performance Metrics', 
                fontsize=18, fontweight='bold', y=0.995)
    plt.tight_layout()
    plt.savefig(image_dir / f'fig1_core_performance_{timescale}.png', dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print(f"     âœ“ Saved: fig1_core_performance_{timescale}.png")
    
    # ============================================================================
    # Summary Statistics Table (ç±»ä¼¼7ç³»åˆ—æ ¼å¼)
    # ============================================================================
    summary_data = {
        'Metric': ['Mean', 'Median', 'Std Dev', 'Min', 'Max'],
        'Correlation': [
            df_single['Correlation'].mean(),
            df_single['Correlation'].median(),
            df_single['Correlation'].std(),
            df_single['Correlation'].min(),
            df_single['Correlation'].max()
        ],
        'MAE (BPS)': [
            df_single['MAE'].mean(),
            df_single['MAE'].median(),
            df_single['MAE'].std(),
            df_single['MAE'].min(),
            df_single['MAE'].max()
        ],
        'RMSE (BPS)': [
            df_single['RMSE'].mean(),
            df_single['RMSE'].median(),
            df_single['RMSE'].std(),
            df_single['RMSE'].min(),
            df_single['RMSE'].max()
        ],
        'RÂ² Score': [
            df_single['RÂ²'].mean(),
            df_single['RÂ²'].median(),
            df_single['RÂ²'].std(),
            df_single['RÂ²'].min(),
            df_single['RÂ²'].max()
        ]
    }
    
    summary_df = pd.DataFrame(summary_data)
    summary_df.to_csv(output_dir / f'summary_statistics_{timescale}.csv', index=False, float_format='%.4f')
    print(f"     âœ“ Saved: summary_statistics_{timescale}.csv")

def generate_final_report(df_results, output_dir, image_dir):
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼šåŒ…å«æ ¸å¿ƒå›¾è¡¨å’Œè¡¨æ ¼"""
    print("\nç”ŸæˆæŠ¥å‘Šå›¾è¡¨å’Œè¡¨æ ¼...")
    
    # ä»æ‰€æœ‰é¢„æµ‹æ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å·²å®Œæˆçš„æ ‡çš„
    print("  ä»é¢„æµ‹æ–‡ä»¶ä¸­è¯»å–æ‰€æœ‰å·²å®Œæˆçš„æ ‡çš„...")
    results = []
    for pred_file in sorted(output_dir.glob('*_predictions.npz')):
        symbol = pred_file.stem.replace('_predictions', '')
        try:
            data = np.load(pred_file)
            pred_10s = data['predictions_10s']
            target_10s = data['targets_10s']
            pred_60s = data['predictions_60s']
            target_60s = data['targets_60s']
            
            # è®¡ç®—æŒ‡æ ‡
            mae_10s = mean_absolute_error(target_10s, pred_10s)
            rmse_10s = np.sqrt(mean_squared_error(target_10s, pred_10s))
            r2_10s = r2_score(target_10s, pred_10s)
            corr_10s = np.corrcoef(target_10s, pred_10s)[0, 1] if len(target_10s) > 1 else 0.0
            
            mae_60s = mean_absolute_error(target_60s, pred_60s)
            rmse_60s = np.sqrt(mean_squared_error(target_60s, pred_60s))
            r2_60s = r2_score(target_60s, pred_60s)
            corr_60s = np.corrcoef(target_60s, pred_60s)[0, 1] if len(target_60s) > 1 else 0.0
            
            results.append({
                'symbol': symbol,
                'mae_10s': mae_10s,
                'rmse_10s': rmse_10s,
                'r2_10s': r2_10s,
                'correlation_10s': corr_10s,
                'mae_60s': mae_60s,
                'rmse_60s': rmse_60s,
                'r2_60s': r2_60s,
                'correlation_60s': corr_60s
            })
        except Exception as e:
            print(f"    âš ï¸  {symbol}: {e}")
    
    # ä½¿ç”¨ä»é¢„æµ‹æ–‡ä»¶è¯»å–çš„æ•°æ®
    df = pd.DataFrame(results)
    print(f"   âœ… è¯»å–åˆ° {len(df)} ä¸ªå·²å®Œæˆçš„æ ‡çš„")
    
    # åˆ›å»ºç»¼åˆæ€§èƒ½æŒ‡æ ‡ï¼ˆå¹³å‡ä¸¤ä¸ªæ—¶é—´å°ºåº¦ï¼‰
    df['MAE_avg'] = (df['mae_10s'] + df['mae_60s']) / 2
    df['RMSE_avg'] = (df['rmse_10s'] + df['rmse_60s']) / 2
    df['RÂ²_avg'] = (df['r2_10s'] + df['r2_60s']) / 2
    df['Correlation_avg'] = (df['correlation_10s'] + df['correlation_60s']) / 2
    
    df = df.sort_values('Correlation_avg', ascending=False).reset_index(drop=True)
    
    # ============================================================================
    # Figure 1: Core Performance Comparison (10s vs 60s)
    # ============================================================================
    print("  1. åˆ›å»ºæ ¸å¿ƒæ€§èƒ½å¯¹æ¯”å›¾è¡¨...")
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1a) Correlation Comparison (10s vs 60s)
    ax = axes[0, 0]
    x_pos = np.arange(len(df))
    width = 0.35
    bars1 = ax.barh(x_pos - width/2, df['correlation_10s'], width, 
                   label='Return 10s', color='#3498db', alpha=0.85, edgecolor='black', linewidth=0.8)
    bars2 = ax.barh(x_pos + width/2, df['correlation_60s'], width, 
                   label='Return 60s', color='#e74c3c', alpha=0.85, edgecolor='black', linewidth=0.8)
    ax.set_yticks(x_pos)
    ax.set_yticklabels(df['symbol'], fontsize=9)
    ax.set_xlabel('Correlation Coefficient', fontweight='bold', fontsize=12)
    ax.set_title('(A) Correlation: 10s vs 60s', fontsize=14, fontweight='bold', pad=12)
    ax.axvline(0, color='black', linestyle='-', linewidth=1.2)
    ax.legend(loc='lower right', fontsize=11, framealpha=0.9)
    ax.grid(True, alpha=0.25, axis='x', linestyle='-', linewidth=0.8)
    
    # 1b) MAE Comparison (10s vs 60s)
    ax = axes[0, 1]
    bars1 = ax.barh(x_pos - width/2, df['mae_10s'], width, 
                   label='Return 10s', color='#3498db', alpha=0.85, edgecolor='black', linewidth=0.8)
    bars2 = ax.barh(x_pos + width/2, df['mae_60s'], width, 
                   label='Return 60s', color='#e74c3c', alpha=0.85, edgecolor='black', linewidth=0.8)
    ax.set_yticks(x_pos)
    ax.set_yticklabels(df['symbol'], fontsize=9)
    ax.set_xlabel('Mean Absolute Error (BPS)', fontweight='bold', fontsize=12)
    ax.set_title('(B) MAE: 10s vs 60s', fontsize=14, fontweight='bold', pad=12)
    ax.legend(loc='lower right', fontsize=11, framealpha=0.9)
    ax.grid(True, alpha=0.25, axis='x', linestyle='-', linewidth=0.8)
    
    # 1c) Correlation Scatter (10s vs 60s)
    ax = axes[1, 0]
    ax.scatter(df['correlation_10s'], df['correlation_60s'], 
              s=150, alpha=0.7, c=df['Correlation_avg'], cmap='RdYlGn', 
              edgecolor='black', linewidth=1.5)
    
    # æ·»åŠ å¯¹è§’çº¿
    min_corr = min(df['correlation_10s'].min(), df['correlation_60s'].min())
    max_corr = max(df['correlation_10s'].max(), df['correlation_60s'].max())
    ax.plot([min_corr, max_corr], [min_corr, max_corr], 'r--', linewidth=2, alpha=0.5, label='y=x')
    
    # æ ‡æ³¨top performers
    for idx, row in df.head(min(3, len(df))).iterrows():
        ax.annotate(row['symbol'], (row['correlation_10s'], row['correlation_60s']), 
                   fontsize=9, fontweight='bold', ha='right', va='bottom',
                   xytext=(-5, 5), textcoords='offset points')
    
    ax.set_xlabel('Correlation (10s)', fontweight='bold', fontsize=12)
    ax.set_ylabel('Correlation (60s)', fontweight='bold', fontsize=12)
    ax.set_title('(C) Correlation: 10s vs 60s Scatter', fontsize=14, fontweight='bold', pad=12)
    ax.legend(fontsize=11, framealpha=0.9)
    ax.grid(True, alpha=0.25, linestyle='-', linewidth=0.8)
    
    # 1d) Average Performance by Symbol
    ax = axes[1, 1]
    colors = ['#27ae60' if x > 0.15 else '#3498db' if x > 0.05 else '#e74c3c' 
              for x in df['Correlation_avg']]
    bars = ax.barh(range(len(df)), df['Correlation_avg'], color=colors, 
                   alpha=0.85, edgecolor='black', linewidth=0.8)
    ax.set_yticks(range(len(df)))
    ax.set_yticklabels(df['symbol'], fontsize=9)
    ax.set_xlabel('Average Correlation Coefficient', fontweight='bold', fontsize=12)
    ax.set_title('(D) Average Correlation (10s & 60s)', fontsize=14, fontweight='bold', pad=12)
    ax.axvline(0, color='black', linestyle='-', linewidth=1.2)
    ax.axvline(df['Correlation_avg'].mean(), color='red', linestyle='--', linewidth=2.5, 
               label=f'Mean: {df["Correlation_avg"].mean():.3f}', alpha=0.8)
    ax.legend(loc='lower right', fontsize=11, framealpha=0.9)
    ax.grid(True, alpha=0.25, axis='x', linestyle='-', linewidth=0.8)
    
    plt.suptitle('DeepLOB-TCN Hierarchical Model - Performance Comparison (10s vs 60s)', 
                fontsize=18, fontweight='bold', y=0.995)
    plt.tight_layout()
    plt.savefig(image_dir / 'fig1_core_performance.png', dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print("     âœ“ Saved: fig1_core_performance.png")
    
    # ============================================================================
    # Figure 2: Summary Statistics (Table Visualization)
    # ============================================================================
    print("  2. åˆ›å»ºæ±‡æ€»ç»Ÿè®¡è¡¨æ ¼...")
    
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    ax.axis('off')
    
    # Calculate statistics for both time scales
    summary_data = {
        'Metric': ['Mean', 'Median', 'Std Dev', 'Min', 'Max'],
        'Corr 10s': [
            df['correlation_10s'].mean(),
            df['correlation_10s'].median(),
            df['correlation_10s'].std(),
            df['correlation_10s'].min(),
            df['correlation_10s'].max()
        ],
        'Corr 60s': [
            df['correlation_60s'].mean(),
            df['correlation_60s'].median(),
            df['correlation_60s'].std(),
            df['correlation_60s'].min(),
            df['correlation_60s'].max()
        ],
        'MAE 10s': [
            df['mae_10s'].mean(),
            df['mae_10s'].median(),
            df['mae_10s'].std(),
            df['mae_10s'].min(),
            df['mae_10s'].max()
        ],
        'MAE 60s': [
            df['mae_60s'].mean(),
            df['mae_60s'].median(),
            df['mae_60s'].std(),
            df['mae_60s'].min(),
            df['mae_60s'].max()
        ],
        'RÂ² 10s': [
            df['r2_10s'].mean(),
            df['r2_10s'].median(),
            df['r2_10s'].std(),
            df['r2_10s'].min(),
            df['r2_10s'].max()
        ],
        'RÂ² 60s': [
            df['r2_60s'].mean(),
            df['r2_60s'].median(),
            df['r2_60s'].std(),
            df['r2_60s'].min(),
            df['r2_60s'].max()
        ]
    }
    
    summary_df = pd.DataFrame(summary_data)
    
    # Create table
    table = ax.table(cellText=summary_df.values,
                    colLabels=summary_df.columns,
                    cellLoc='center',
                    loc='center',
                    bbox=[0, 0, 1, 1])
    
    table.auto_set_font_size(False)
    table.set_fontsize(11)
    table.scale(1, 2.2)
    
    # Style the table
    for i in range(len(summary_df.columns)):
        table[(0, i)].set_facecolor('#3498db')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    for i in range(1, len(summary_df) + 1):
        for j in range(len(summary_df.columns)):
            if i % 2 == 0:
                table[(i, j)].set_facecolor('#ecf0f1')
            else:
                table[(i, j)].set_facecolor('white')
            table[(i, j)].set_text_props(weight='normal')
    
    # Format numbers
    for i in range(1, len(summary_df) + 1):
        for j in range(1, len(summary_df.columns)):
            val = summary_df.iloc[i-1, j]
            if j <= 2:  # Correlation columns
                table[(i, j)].get_text().set_text(f'{val:.4f}')
            elif j <= 4:  # MAE columns
                table[(i, j)].get_text().set_text(f'{val:.2f}')
            else:  # RÂ² columns
                table[(i, j)].get_text().set_text(f'{val:.4f}')
    
    ax.set_title('Summary Statistics (Hierarchical Model)', fontsize=18, fontweight='bold', pad=20)
    plt.savefig(image_dir / 'fig2_summary_statistics.png', dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print("     âœ“ Saved: fig2_summary_statistics.png")
    
    # ============================================================================
    # Figure 3: Top Performers (Table Visualization)
    # ============================================================================
    print("  3. åˆ›å»ºTop Performersè¡¨æ ¼...")
    
    fig, ax = plt.subplots(1, 1, figsize=(14, 8))
    ax.axis('off')
    
    top_n = df.copy()
    top_n.insert(0, 'Rank', range(1, len(top_n) + 1))
    top_n_display = top_n[['Rank', 'symbol', 'correlation_10s', 'correlation_60s', 
                           'mae_10s', 'mae_60s', 'Correlation_avg']].copy()
    
    # Format numbers
    top_n_display['correlation_10s'] = top_n_display['correlation_10s'].apply(lambda x: f'{x:.4f}')
    top_n_display['correlation_60s'] = top_n_display['correlation_60s'].apply(lambda x: f'{x:.4f}')
    top_n_display['mae_10s'] = top_n_display['mae_10s'].apply(lambda x: f'{x:.2f}')
    top_n_display['mae_60s'] = top_n_display['mae_60s'].apply(lambda x: f'{x:.2f}')
    top_n_display['Correlation_avg'] = top_n_display['Correlation_avg'].apply(lambda x: f'{x:.4f}')
    
    # Create table
    table = ax.table(cellText=top_n_display.values,
                    colLabels=top_n_display.columns,
                    cellLoc='center',
                    loc='center',
                    bbox=[0, 0, 1, 1])
    
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2.0)
    
    # Style the table
    for i in range(len(top_n_display.columns)):
        table[(0, i)].set_facecolor('#27ae60')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    for i in range(1, len(top_n_display) + 1):
        for j in range(len(top_n_display.columns)):
            if i % 2 == 0:
                table[(i, j)].set_facecolor('#ecf0f1')
            else:
                table[(i, j)].set_facecolor('white')
            table[(i, j)].set_text_props(weight='normal')
        
        # Highlight top 3
        if i <= min(3, len(top_n_display)):
            for j in range(len(top_n_display.columns)):
                table[(i, j)].set_facecolor('#d5f4e6')
    
    ax.set_title(f'Top {len(top_n)} Performers by Average Correlation', fontsize=18, fontweight='bold', pad=20)
    plt.savefig(image_dir / 'fig3_top_performers.png', dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print("     âœ“ Saved: fig3_top_performers.png")
    
    # Save CSV tables
    summary_df.to_csv(output_dir / 'summary_statistics.csv', index=False, float_format='%.4f')
    top_n[['Rank', 'symbol', 'correlation_10s', 'correlation_60s', 'mae_10s', 'mae_60s', 
           'Correlation_avg']].to_csv(
        output_dir / 'top_performers.csv', index=False, float_format='%.4f')
    print("     âœ“ Saved: summary_statistics.csv")
    print("     âœ“ Saved: top_performers.csv")
    
    # ============================================================================
    # ä¸º10så’Œ60såˆ†åˆ«ç”Ÿæˆç±»ä¼¼7ç³»åˆ—çš„å›¾è¡¨å’Œè¡¨æ ¼
    # ============================================================================
    
    # ç”ŸæˆReturn 10sçš„å›¾è¡¨å’Œè¡¨æ ¼
    print("\n  4. ç”ŸæˆReturn 10sçš„å›¾è¡¨å’Œè¡¨æ ¼...")
    generate_single_timescale_report(df, '10s', output_dir, image_dir)
    
    # ç”ŸæˆReturn 60sçš„å›¾è¡¨å’Œè¡¨æ ¼
    print("\n  5. ç”ŸæˆReturn 60sçš„å›¾è¡¨å’Œè¡¨æ ¼...")
    generate_single_timescale_report(df, '60s', output_dir, image_dir)
    
    print("\nâœ… æŠ¥å‘Šå›¾è¡¨å’Œè¡¨æ ¼ç”Ÿæˆå®Œæˆï¼")


# ============================================================================
# 6. Main Function
# ============================================================================

def main():
    print("="*80)
    print("ğŸš€ DeepLOB-TCN Hierarchical Modeling - Multi-Timescale Prediction (Optimized)")
    print("="*80)
    print("\nç­–ç•¥: æ¯ä¸ªæ ‡çš„ç‹¬ç«‹è®­ç»ƒï¼Œæ”¯æŒå¤šGPUå¹¶è¡Œ")
    print("æ•°æ®: 10å¤©æ•°æ® (2025-08-01 to 2025-08-10)")
    print("æ¨¡å‹: DeepLOB-TCN Hierarchical (åŒæ—¶é¢„æµ‹return_10så’Œreturn_60s)")
    print("æ”¹è¿›: Hierarchicalæ¶æ„ + EMAå¹³æ»‘ + å¤šä»»åŠ¡å­¦ä¹ ")
    print("ä¼˜åŒ–: äº¤å‰æ³¨æ„åŠ› + é—¨æ§èåˆ + æ®‹å·®è¿æ¥ + è‡ªé€‚åº”æŸå¤±æƒé‡")
    
    # é…ç½®
    config = {
        'sequence_length': 100,
        'batch_size': 2048,
        'num_workers': 2,
        'learning_rate': 0.001,
        'num_epochs': 20,
        'dropout': 0.3,
        'early_stopping_patience': 5,
        'ema_alpha': 0.2,  # EMAå¹³æ»‘å› å­
        'loss_weight_10s': 0.5,  # return_10sæŸå¤±æƒé‡ï¼ˆåˆå§‹å€¼ï¼‰
        'loss_weight_60s': 0.5,  # return_60sæŸå¤±æƒé‡ï¼ˆåˆå§‹å€¼ï¼‰
        'short_term_channels': [64, 64, 64, 64],  # çŸ­æœŸTCNé€šé“æ•°
        'long_term_channels': [64, 64, 64, 64, 64],  # é•¿æœŸTCNé€šé“æ•°ï¼ˆæ›´æ·±ï¼‰
        # ä¼˜åŒ–é€‰é¡¹
        'use_attention': True,  # ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
        'use_gated_fusion': True,  # ä½¿ç”¨é—¨æ§èåˆæœºåˆ¶
        'use_residual': True,  # ä½¿ç”¨æ®‹å·®è¿æ¥
        'adaptive_loss_weight': False  # è‡ªé€‚åº”æŸå¤±æƒé‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤å…³é—­ï¼‰
    }
    
    # è·¯å¾„ - æ‰€æœ‰è¾“å‡ºä¿å­˜åˆ°8_modelsæ–‡ä»¶å¤¹
    data_dir = Path('data_250801_250810')
    output_dir = Path('8_models')
    log_dir = output_dir / 'log'
    image_dir = output_dir / 'image'
    
    output_dir.mkdir(exist_ok=True)
    log_dir.mkdir(exist_ok=True)
    image_dir.mkdir(exist_ok=True)
    
    # è¯»å–æ‰€æœ‰æ ‡çš„
    metadata_file = data_dir / 'metadata.json'
    if metadata_file.exists():
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
        all_symbols = metadata.get('symbols', [])
    else:
        npy_files = sorted(data_dir.glob('*_20250801_20250810.npy'))
        all_symbols = [f.stem.replace('_20250801_20250810', '') for f in npy_files]
    
    # âœ… è¿‡æ»¤å·²å®Œæˆçš„æ ‡çš„ï¼ˆæ£€æŸ¥predictions.npzæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼‰
    target_symbols = []
    skipped_symbols = []
    for symbol in all_symbols:
        predictions_file = output_dir / f"{symbol}_predictions.npz"
        if predictions_file.exists():
            skipped_symbols.append(symbol)
        else:
            target_symbols.append(symbol)
    
    print(f"\næ‰¾åˆ° {len(all_symbols)} ä¸ªæ ‡çš„")
    print(f"  å·²è·³è¿‡ {len(skipped_symbols)} ä¸ªå·²å®Œæˆçš„æ ‡çš„: {', '.join(skipped_symbols) if skipped_symbols else 'æ— '}")
    print(f"  å¾…è®­ç»ƒ {len(target_symbols)} ä¸ªæ ‡çš„")
    if target_symbols:
        print(f"  å¾…è®­ç»ƒæ ‡çš„: {', '.join(target_symbols)}")
    print(f"é…ç½®: {json.dumps(config, indent=2)}")
    print(f"æ—¥å¿—ç›®å½•: {log_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"âœ… EMAå¹³æ»‘å› å­: {config['ema_alpha']}")
    print(f"âœ… æŸå¤±æƒé‡: 10s={config['loss_weight_10s']}, 60s={config['loss_weight_60s']}")
    print(f"âœ… ä¼˜åŒ–é€‰é¡¹:")
    print(f"   - äº¤å‰æ³¨æ„åŠ›: {config.get('use_attention', True)}")
    print(f"   - é—¨æ§èåˆ: {config.get('use_gated_fusion', True)}")
    print(f"   - æ®‹å·®è¿æ¥: {config.get('use_residual', True)}")
    print(f"   - è‡ªé€‚åº”æŸå¤±æƒé‡: {config.get('adaptive_loss_weight', False)}")
    
    if not target_symbols:
        print(f"\nâœ… æ‰€æœ‰æ ‡çš„å·²å®Œæˆè®­ç»ƒï¼Œæ— éœ€è®­ç»ƒ")
        return
    
    print(f"\nğŸ”„ å¼€å§‹è®­ç»ƒ {len(target_symbols)} ä¸ªæ ‡çš„...")
    
    # å‡†å¤‡å¹¶è¡Œè®­ç»ƒå‚æ•°
    n_gpus = torch.cuda.device_count()
    print(f"\nå¯ç”¨GPUæ•°é‡: {n_gpus}")
    
    if n_gpus >= 4:
        max_workers = 4  # ä½¿ç”¨GPU 0ã€1ã€2ã€3ï¼Œæ¯å¼ GPU 1ä¸ªè¿›ç¨‹
        print(f"âœ… å°†ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œè®­ç»ƒ (GPU 0ã€1ã€2ã€3ï¼Œæ¯å¼ GPU 1ä¸ªè¿›ç¨‹)")
    elif n_gpus >= 2:
        max_workers = min(4, n_gpus)
        print(f"âœ… å°†ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œè®­ç»ƒ (GPU 0ã€1ï¼Œæ¯å¼ GPU {max_workers//2}ä¸ªè¿›ç¨‹)")
    else:
        max_workers = min(4, n_gpus)
        print(f"âœ… å°†ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å…±äº«1ä¸ªGPUå¹¶è¡Œè®­ç»ƒ")
    
    print(f"   Batch Size: {config['batch_size']}")
    
    # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
    tasks = []
    for i, symbol in enumerate(target_symbols):
        if n_gpus >= 4:
            gpu_id = i % 4
        elif n_gpus >= 2:
            gpu_id = i % 2
        else:
            gpu_id = 0
        tasks.append((symbol, data_dir, output_dir, log_dir, config, gpu_id))
        print(f"   {symbol} -> GPU {gpu_id}")
    
    results = []
    failed_symbols = []
    
    if tasks:
        # å¹¶è¡Œè®­ç»ƒ
        print(f"\n{'='*80}")
        print(f"ğŸ‹ï¸  å¼€å§‹å¹¶è¡Œè®­ç»ƒ ({max_workers} ä¸ªè¿›ç¨‹)")
        print(f"{'='*80}\n")
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            future_to_symbol = {executor.submit(train_single_symbol_worker, task): task[0] 
                               for task in tasks}
            
            completed = 0
            for future in as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                completed += 1
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                        print(f"[{completed}/{len(tasks)}] âœ… {symbol} å®Œæˆ")
                    else:
                        failed_symbols.append(symbol)
                        print(f"[{completed}/{len(tasks)}] âŒ {symbol} å¤±è´¥")
                except Exception as e:
                    failed_symbols.append(symbol)
                    print(f"[{completed}/{len(tasks)}] âŒ {symbol} å¼‚å¸¸: {e}")
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    print(f"\n{'='*80}")
    print(f"ğŸ“Š è®­ç»ƒæ±‡æ€»")
    print(f"{'='*80}")
    
    if results:
        df_results = pd.DataFrame(results)
        df_results = df_results.sort_values('correlation_10s', ascending=False)
    
        # ä¿å­˜CSV
        csv_file = output_dir / 'training_summary_hierarchical.csv'
        df_results.to_csv(csv_file, index=False)
        print(f"\nâœ… æ±‡æ€»è¡¨æ ¼å·²ä¿å­˜: {csv_file}")
        
        # æ‰“å°ç»Ÿè®¡
        print(f"\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
        print(f"   æˆåŠŸè®­ç»ƒ: {len(results)}/{len(target_symbols)}")
        if len(results) > 0:
            print(f"   Return 10s - å¹³å‡ MAE:  {df_results['mae_10s'].mean():.6f}, å¹³å‡ Corr: {df_results['correlation_10s'].mean():.6f}")
            print(f"   Return 60s - å¹³å‡ MAE:  {df_results['mae_60s'].mean():.6f}, å¹³å‡ Corr: {df_results['correlation_60s'].mean():.6f}")
    
        print(f"\nğŸ† Top 10 æ ‡çš„ (æŒ‰Correlation_10sæ’åº):")
        print(df_results[['symbol', 'correlation_10s', 'correlation_60s', 'mae_10s', 'mae_60s']].head(10).to_string(index=False))
        
        # ============================================================================
        # ç”ŸæˆæŠ¥å‘Šå›¾è¡¨å’Œè¡¨æ ¼
        # ============================================================================
        print(f"\n{'='*80}")
        print(f"ğŸ“Š ç”ŸæˆæŠ¥å‘Šå›¾è¡¨å’Œè¡¨æ ¼")
        print(f"{'='*80}")
        
        generate_final_report(df_results, output_dir, image_dir)
    
    if failed_symbols:
        print(f"\nâŒ å¤±è´¥çš„æ ‡çš„: {failed_symbols}")
    
    print(f"\n{'='*80}")
    print(f"âœ… æ‰€æœ‰è®­ç»ƒå®Œæˆ!")
    print(f"{'='*80}")


if __name__ == '__main__':
    main()

